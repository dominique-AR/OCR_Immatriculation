{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGLBrzF8hKgS"
      },
      "source": [
        "## Switch to CPU Instance (Advisable only for Non Colab-Pro instance)\n",
        "\n",
        "1. Switch to CPU Instance for until Step 2 for non GPU dependent tasks\n",
        "2. This increases your time available for the GPU dependent tasks on a Colab instance\n",
        "2. Change Runtime type to CPU by Runtime(Top Left tab)->Change Runtime Type->None(Hardware Accelerator)\n",
        "3.   Then click on Connect (Top Right)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjpjyNg5c2V9"
      },
      "source": [
        "## Mounting Google drive\n",
        "Mount your Google drive storage to this Colab instance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EvUVkYw0hzqG",
        "outputId": "64b949c0-c2e0-4d9c-b809-7631fc394bf2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: GOOGLE_COLAB=1\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    %env GOOGLE_COLAB=1\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "except:\n",
        "    %env GOOGLE_COLAB=0\n",
        "    print(\"Warning: Not a Colab Environment\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IZtVf3cPyom"
      },
      "source": [
        "# License Plate Recognition using TAO LPRNet\n",
        "\n",
        "Transfer learning is the process of transferring learned features from one application to another. It is a commonly used training technique where you use a model trained on one task and re-train to use it on a different task. \n",
        "\n",
        "Train Adapt Optimize (TAO) Toolkit  is a simple and easy-to-use Python based AI toolkit for taking purpose-built AI models and customizing them with users' own data.\n",
        "\n",
        "<img align=\"center\" src=\"https://developer.nvidia.com/sites/default/files/akamai/TAO/tlt-tao-toolkit-bring-your-own-model-diagram.png\" width=\"1080\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxZQEfTBPyoq"
      },
      "source": [
        "## Learning Objectives\n",
        "\n",
        "In this notebook, you will learn how to leverage the simplicity and convenience of TAO to:\n",
        "\n",
        "* Take a pretrained baseline18 LPRNet model and train it on the OpenALPR benchmark dataset\n",
        "* Run Inference on the trained model\n",
        "* Export the trained model to a .etlt file for deployment to DeepStream\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "This notebook shows an example usecase of LPRNet using Train Adapt Optimize (TAO) Toolkit.\n",
        "\n",
        "0. [Set up env variables](#head-0)\n",
        "1. [Prepare dataset and pre-trained model](#head-1) <br>\n",
        "    1.1 [Download pre-trained model](#head-1-1) <br>\n",
        "2. [Setup GPU environment](#head-2) <br>\n",
        "    2.1 [Connect to GPU Instance](#head-2-1) <br>\n",
        "    2.2 [Mounting Google drive](#head-2-2) <br>\n",
        "    2.3 [Setup Python environment](#head-2-3) <br>\n",
        "    2.4 [Reset env variables](#head-2-4) <br>\n",
        "3. [Provide training specification](#head-3)\n",
        "4. [Run TAO training](#head-4)\n",
        "5. [Evaluate trained models](#head-5)\n",
        "6. [Inferences](#head-6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5lxlv5IPyoq"
      },
      "source": [
        "#### Note\n",
        "1. This notebook currently is by default set up to run training using 1 GPU. To use more GPU's please update the env variable `$NUM_GPUS` accordingly\n",
        "2. This notebook uses OPENALPR dataset by default, which should be around ~2.2 MB.\n",
        "3. Using the default config/spec file provided in this notebook, each weight file size of lprnet created during training will be ~111 MB\n",
        "\n",
        "## 0. Set up env variables and set FIXME parameters <a class=\"anchor\" id=\"head-0\"></a>\n",
        "\n",
        "*Note: This notebook currently is by default set up to run training using 1 GPU. To use more GPU's please update the env variable `$NUM_GPUS` accordingly*\n",
        "\n",
        "#### FIXME\n",
        "1. NUM_GPUS - set this to <= number of GPU's availble on the instance\n",
        "1. GPU_INDEX - set to to the indices of the GPU available on the instance\n",
        "1. COLAB_NOTEBOOKS_PATH - for Google Colab environment, set this path where you want to clone the repo to; for local system environment, set this path to the already cloned repo\n",
        "1. EXPERIMENT_DIR - set this path to a folder location where pretrained models, checkpoints and log files during different model actions will be saved\n",
        "1. delete_existing_experiments - set to True to remove existing pretrained models, checkpoints and log files of a previous experiment\n",
        "1. DATA_DIR - set this path to a folder location where you want to dataset to be present\n",
        "1. delete_existing_data - set this to True to remove existing preprocessed and original data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dXT1Uv__Pyor",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cebc31a2-6490-4d53-c519-77e9be22f97b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: TAO_DOCKER_DISABLE=1\n",
            "env: KEY=nvidia_tlt\n",
            "env: NUM_GPUS=1\n",
            "env: GPU_INDEX=0\n",
            "env: COLAB_NOTEBOOKS_PATH=/content/drive/MyDrive/nvidia-tao\n",
            "Cloning into '/content/drive/MyDrive/nvidia-tao'...\n",
            "remote: Enumerating objects: 2281, done.\u001b[K\n",
            "remote: Counting objects: 100% (961/961), done.\u001b[K\n",
            "remote: Compressing objects: 100% (579/579), done.\u001b[K\n",
            "remote: Total 2281 (delta 614), reused 712 (delta 372), pack-reused 1320\u001b[K\n",
            "Receiving objects: 100% (2281/2281), 3.99 MiB | 15.23 MiB/s, done.\n",
            "Resolving deltas: 100% (1431/1431), done.\n",
            "env: EXPERIMENT_DIR=/content/drive/MyDrive/results/lprnet\n",
            "env: DATA_DIR=/content/drive/MyDrive/lprnet_data/\n",
            "env: SPECS_DIR=/content/drive/MyDrive/nvidia-tao/tensorflow/lprnet/specs\n",
            "total 2\n",
            "-rw------- 1 root root   70 Jan 30 08:19 us_lp_characters.txt\n",
            "-rw------- 1 root root 1137 Jan 30 08:19 tutorial_spec.txt\n"
          ]
        }
      ],
      "source": [
        "# Setting up env variables for cleaner command line commands.\n",
        "import os\n",
        "\n",
        "%env TAO_DOCKER_DISABLE=1\n",
        "\n",
        "%env KEY=nvidia_tlt\n",
        "#FIXME1\n",
        "%env NUM_GPUS=1\n",
        "#FIXME2\n",
        "%env GPU_INDEX=0\n",
        "\n",
        "#FIXME3\n",
        "%env COLAB_NOTEBOOKS_PATH=/content/drive/MyDrive/nvidia-tao\n",
        "if os.environ[\"GOOGLE_COLAB\"] == \"1\":\n",
        "    if not os.path.exists(os.path.join(os.environ[\"COLAB_NOTEBOOKS_PATH\"])):\n",
        "      !git clone https://github.com/NVIDIA-AI-IOT/nvidia-tao.git $COLAB_NOTEBOOKS_PATH\n",
        "else:\n",
        "    if not os.path.exists(os.environ[\"COLAB_NOTEBOOKS_PATH\"]):\n",
        "        raise Exception(\"Error, enter the path of the colab notebooks repo correctly\")\n",
        "\n",
        "#FIXME4\n",
        "%env EXPERIMENT_DIR=/content/drive/MyDrive/results/lprnet\n",
        "#FIXME5\n",
        "delete_existing_experiments = True\n",
        "#FIXME6\n",
        "%env DATA_DIR=/content/drive/MyDrive/lprnet_data/\n",
        "#FIXME7\n",
        "delete_existing_data = False\n",
        "\n",
        "if delete_existing_experiments:\n",
        "    !sudo rm -rf $EXPERIMENT_DIR\n",
        "if delete_existing_data:\n",
        "    !sudo rm -rf $DATA_DIR\n",
        "\n",
        "SPECS_DIR=f\"{os.environ['COLAB_NOTEBOOKS_PATH']}/tensorflow/lprnet/specs\"\n",
        "%env SPECS_DIR={SPECS_DIR}\n",
        "# Showing list of specification files.\n",
        "!ls -rlt $SPECS_DIR\n",
        "\n",
        "!sudo mkdir -p $DATA_DIR && sudo chmod -R 777 $DATA_DIR\n",
        "!sudo mkdir -p $EXPERIMENT_DIR && sudo chmod -R 777 $EXPERIMENT_DIR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WI_8N9_IPyov"
      },
      "source": [
        "## 1. Prepare dataset and pre-trained model <a class=\"anchor\" id=\"head-1\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvQbbU5wPyov"
      },
      "source": [
        " We will be using the OpenALPR benchmark dataset for the tutorial. The following script will download the dataset automatically and convert it to the format used by TAO. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "kBI24bHSPyov",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "335325d8-1ef6-4174-8438-307ffbeef044"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+ '[' -z /content/drive/MyDrive/lprnet_data/ ']'\n",
            "++ pwd\n",
            "+ CURRENT_DIR=/content\n",
            "+ echo 'Cloning OpenALPR benchmark directory'\n",
            "Cloning OpenALPR benchmark directory\n",
            "+ '[' '!' -e benchmarks ']'\n",
            "+ git clone https://github.com/openalpr/benchmarks benchmarks\n",
            "Cloning into 'benchmarks'...\n",
            "remote: Enumerating objects: 1752, done.\u001b[K\n",
            "remote: Total 1752 (delta 0), reused 0 (delta 0), pack-reused 1752\n",
            "Receiving objects: 100% (1752/1752), 187.98 MiB | 31.15 MiB/s, done.\n",
            "Resolving deltas: 100% (34/34), done.\n",
            "+ OUTPUT_DIR=/content/drive/MyDrive/lprnet_data\n",
            "+ mkdir -p /content/drive/MyDrive/lprnet_data\n",
            "+++ readlink -f /content/drive/MyDrive/nvidia-tao/tensorflow/lprnet/download_and_prepare_data.sh\n",
            "++ dirname /content/drive/MyDrive/nvidia-tao/tensorflow/lprnet/download_and_prepare_data.sh\n",
            "+ SCRIPT_DIR=/content/drive/MyDrive/nvidia-tao/tensorflow/lprnet\n",
            "+ echo 'Preprocessing OpenALPR benchmarks data for US'\n",
            "Preprocessing OpenALPR benchmarks data for US\n",
            "+ python3 /content/drive/MyDrive/nvidia-tao/tensorflow/lprnet/preprocess_openalpr_benchmark.py --input_dir=/content/benchmarks/endtoend/us/ --output_dir=/content/drive/MyDrive/lprnet_data\n",
            "Total 222 samples in benchmark dataset\n",
            "111 for train and 111 for val\n"
          ]
        }
      ],
      "source": [
        "!bash $COLAB_NOTEBOOKS_PATH/tensorflow/lprnet/download_and_prepare_data.sh $DATA_DIR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Oi9ACFA0Pyov",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4edc061-695c-401e-8219-9f4aa49cc21d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/lprnet_data/\n",
            "total 8\n",
            "drwx------ 4 root root 4096 Jan 30 08:20 train\n",
            "drwx------ 4 root root 4096 Jan 30 08:20 val\n",
            "total 8\n",
            "drwx------ 2 root root 4096 Jan 30 08:20 image\n",
            "drwx------ 2 root root 4096 Jan 30 08:20 label\n",
            "total 527\n",
            "-rw------- 1 root root  1667 Jan 30 08:20 12c6cb72-3ea3-49e7-b381-e0cdfc5e8960.jpg\n",
            "-rw------- 1 root root  1882 Jan 30 08:20 1e241dc8-8f18-4955-8988-03a0ab49f813.jpg\n",
            "-rw------- 1 root root  2092 Jan 30 08:20 22e54a62-57a8-4a0a-88c1-4b9758f67651.jpg\n",
            "-rw------- 1 root root 10710 Jan 30 08:20 316b64c0-55bf-4079-a1c0-d93f461a576f.jpg\n",
            "-rw------- 1 root root  2092 Jan 30 08:20 37170dd1-2802-4e38-b982-c5d07c64ff67.jpg\n",
            "-rw------- 1 root root  2067 Jan 30 08:20 4be2025c-09f7-4bb0-b1bd-8e8633e6dec1.jpg\n",
            "-rw------- 1 root root  1860 Jan 30 08:20 5b562a61-34ad-4f00-9164-d34abb7a38e4.jpg\n",
            "-rw------- 1 root root  2096 Jan 30 08:20 c9368c55-210d-456c-a5ef-c310e60039ec.jpg\n",
            "-rw------- 1 root root 14111 Jan 30 08:20 car13.jpg\n",
            "-rw------- 1 root root  3431 Jan 30 08:20 car14.jpg\n",
            "-rw------- 1 root root  4161 Jan 30 08:20 car15.jpg\n",
            "-rw------- 1 root root  2911 Jan 30 08:20 car16.jpg\n",
            "-rw------- 1 root root  1748 Jan 30 08:20 car17.jpg\n",
            "-rw------- 1 root root 25056 Jan 30 08:20 car19.jpg\n",
            "-rw------- 1 root root  5296 Jan 30 08:20 car1.jpg\n",
            "-rw------- 1 root root  2243 Jan 30 08:20 car20.jpg\n",
            "-rw------- 1 root root  1650 Jan 30 08:20 car22.jpg\n",
            "-rw------- 1 root root  5366 Jan 30 08:20 car3.jpg\n",
            "-rw------- 1 root root  3306 Jan 30 08:20 car5.jpg\n",
            "-rw------- 1 root root  2748 Jan 30 08:20 car6.jpg\n",
            "-rw------- 1 root root 11377 Jan 30 08:20 car7.jpg\n",
            "-rw------- 1 root root  2537 Jan 30 08:20 car8.jpg\n",
            "-rw------- 1 root root  6907 Jan 30 08:20 car9-0.jpg\n",
            "-rw------- 1 root root 11997 Jan 30 08:20 car9-1.jpg\n",
            "-rw------- 1 root root  3095 Jan 30 08:20 car9-2.jpg\n",
            "-rw------- 1 root root  4836 Jan 30 08:20 car9-4.jpg\n",
            "-rw------- 1 root root  6960 Jan 30 08:20 car9-5.jpg\n",
            "-rw------- 1 root root  6260 Jan 30 08:20 car9-7.jpg\n",
            "-rw------- 1 root root  3626 Jan 30 08:20 car9-9.jpg\n",
            "-rw------- 1 root root  3125 Jan 30 08:20 car9.jpg\n",
            "-rw------- 1 root root  1974 Jan 30 08:20 cfaa9dd2-a388-4e92-bb3a-ae65c28d8139.jpg\n",
            "-rw------- 1 root root  1918 Jan 30 08:20 f8fc5e59-9083-466b-ae3f-6b869a0b257b.jpg\n",
            "-rw------- 1 root root 20384 Jan 30 08:20 us10.jpg\n",
            "-rw------- 1 root root  5125 Jan 30 08:20 us1.jpg\n",
            "-rw------- 1 root root  4663 Jan 30 08:20 us2.jpg\n",
            "-rw------- 1 root root  4186 Jan 30 08:20 us3.jpg\n",
            "-rw------- 1 root root  9130 Jan 30 08:20 us4.jpg\n",
            "-rw------- 1 root root 13834 Jan 30 08:20 us5.jpg\n",
            "-rw------- 1 root root 21030 Jan 30 08:20 us7.jpg\n",
            "-rw------- 1 root root  2651 Jan 30 08:20 wts-lg-000021.jpg\n",
            "-rw------- 1 root root  2397 Jan 30 08:20 wts-lg-000022.jpg\n",
            "-rw------- 1 root root  2762 Jan 30 08:20 wts-lg-000023.jpg\n",
            "-rw------- 1 root root  3010 Jan 30 08:20 wts-lg-000025.jpg\n",
            "-rw------- 1 root root  6964 Jan 30 08:20 wts-lg-000027.jpg\n",
            "-rw------- 1 root root  2198 Jan 30 08:20 wts-lg-000035.jpg\n",
            "-rw------- 1 root root  1882 Jan 30 08:20 wts-lg-000036.jpg\n",
            "-rw------- 1 root root  2849 Jan 30 08:20 wts-lg-000038.jpg\n",
            "-rw------- 1 root root  3907 Jan 30 08:20 wts-lg-000041.jpg\n",
            "-rw------- 1 root root  2609 Jan 30 08:20 wts-lg-000042.jpg\n",
            "-rw------- 1 root root  2309 Jan 30 08:20 wts-lg-000050.jpg\n",
            "-rw------- 1 root root  4337 Jan 30 08:20 wts-lg-000051.jpg\n",
            "-rw------- 1 root root  1929 Jan 30 08:20 wts-lg-000055.jpg\n",
            "-rw------- 1 root root  3535 Jan 30 08:20 wts-lg-000060.jpg\n",
            "-rw------- 1 root root  2670 Jan 30 08:20 wts-lg-000061.jpg\n",
            "-rw------- 1 root root  3184 Jan 30 08:20 wts-lg-000062.jpg\n",
            "-rw------- 1 root root  2590 Jan 30 08:20 wts-lg-000063.jpg\n",
            "-rw------- 1 root root  3173 Jan 30 08:20 wts-lg-000064.jpg\n",
            "-rw------- 1 root root  2084 Jan 30 08:20 wts-lg-000072.jpg\n",
            "-rw------- 1 root root  3319 Jan 30 08:20 wts-lg-000073.jpg\n",
            "-rw------- 1 root root  3603 Jan 30 08:20 wts-lg-000075.jpg\n",
            "-rw------- 1 root root  3732 Jan 30 08:20 wts-lg-000079.jpg\n",
            "-rw------- 1 root root  5053 Jan 30 08:20 wts-lg-000080.jpg\n",
            "-rw------- 1 root root  4483 Jan 30 08:20 wts-lg-000086.jpg\n",
            "-rw------- 1 root root  4604 Jan 30 08:20 wts-lg-000087.jpg\n",
            "-rw------- 1 root root  1887 Jan 30 08:20 wts-lg-000088.jpg\n",
            "-rw------- 1 root root  3370 Jan 30 08:20 wts-lg-000090.jpg\n",
            "-rw------- 1 root root  2293 Jan 30 08:20 wts-lg-000091.jpg\n",
            "-rw------- 1 root root  2953 Jan 30 08:20 wts-lg-000096.jpg\n",
            "-rw------- 1 root root  3838 Jan 30 08:20 wts-lg-000098.jpg\n",
            "-rw------- 1 root root  3725 Jan 30 08:20 wts-lg-000099.jpg\n",
            "-rw------- 1 root root  2304 Jan 30 08:20 wts-lg-000113.jpg\n",
            "-rw------- 1 root root  3483 Jan 30 08:20 wts-lg-000115.jpg\n",
            "-rw------- 1 root root  3464 Jan 30 08:20 wts-lg-000119.jpg\n",
            "-rw------- 1 root root  3442 Jan 30 08:20 wts-lg-000120.jpg\n",
            "-rw------- 1 root root  2286 Jan 30 08:20 wts-lg-000121.jpg\n",
            "-rw------- 1 root root  2021 Jan 30 08:20 wts-lg-000122.jpg\n",
            "-rw------- 1 root root  2512 Jan 30 08:20 wts-lg-000124.jpg\n",
            "-rw------- 1 root root  2778 Jan 30 08:20 wts-lg-000125.jpg\n",
            "-rw------- 1 root root  3239 Jan 30 08:20 wts-lg-000127.jpg\n",
            "-rw------- 1 root root  3027 Jan 30 08:20 wts-lg-000128.jpg\n",
            "-rw------- 1 root root  3956 Jan 30 08:20 wts-lg-000130.jpg\n",
            "-rw------- 1 root root  3119 Jan 30 08:20 wts-lg-000131.jpg\n",
            "-rw------- 1 root root  3492 Jan 30 08:20 wts-lg-000133.jpg\n",
            "-rw------- 1 root root  6167 Jan 30 08:20 wts-lg-000135.jpg\n",
            "-rw------- 1 root root  7349 Jan 30 08:20 wts-lg-000137.jpg\n",
            "-rw------- 1 root root  5291 Jan 30 08:20 wts-lg-000138.jpg\n",
            "-rw------- 1 root root  3228 Jan 30 08:20 wts-lg-000140.jpg\n",
            "-rw------- 1 root root  2880 Jan 30 08:20 wts-lg-000141.jpg\n",
            "-rw------- 1 root root  7493 Jan 30 08:20 wts-lg-000144.jpg\n",
            "-rw------- 1 root root  2734 Jan 30 08:20 wts-lg-000146.jpg\n",
            "-rw------- 1 root root  2876 Jan 30 08:20 wts-lg-000147.jpg\n",
            "-rw------- 1 root root  3751 Jan 30 08:20 wts-lg-000149.jpg\n",
            "-rw------- 1 root root  3140 Jan 30 08:20 wts-lg-000152.jpg\n",
            "-rw------- 1 root root  3503 Jan 30 08:20 wts-lg-000154.jpg\n",
            "-rw------- 1 root root  6083 Jan 30 08:20 wts-lg-000155.jpg\n",
            "-rw------- 1 root root  7538 Jan 30 08:20 wts-lg-000158.jpg\n",
            "-rw------- 1 root root  7254 Jan 30 08:20 wts-lg-000164.jpg\n",
            "-rw------- 1 root root  4216 Jan 30 08:20 wts-lg-000167.jpg\n",
            "-rw------- 1 root root  4709 Jan 30 08:20 wts-lg-000169.jpg\n",
            "-rw------- 1 root root  3655 Jan 30 08:20 wts-lg-000172.jpg\n",
            "-rw------- 1 root root  7503 Jan 30 08:20 wts-lg-000175.jpg\n",
            "-rw------- 1 root root  8097 Jan 30 08:20 wts-lg-000176.jpg\n",
            "-rw------- 1 root root  3419 Jan 30 08:20 wts-lg-000178.jpg\n",
            "-rw------- 1 root root  2441 Jan 30 08:20 wts-lg-000179.jpg\n",
            "-rw------- 1 root root  1933 Jan 30 08:20 wts-lg-000183.jpg\n",
            "-rw------- 1 root root  3435 Jan 30 08:20 wts-lg-000186.jpg\n",
            "-rw------- 1 root root  2049 Jan 30 08:20 wts-lg-000189.jpg\n",
            "-rw------- 1 root root  7421 Jan 30 08:20 wts-lg-000191.jpg\n",
            "-rw------- 1 root root  3808 Jan 30 08:20 wts-lg-000192.jpg\n",
            "-rw------- 1 root root  6452 Jan 30 08:20 wts-lg-000196.jpg\n",
            "-rw------- 1 root root  2671 Jan 30 08:20 wts-lg-000197.jpg\n",
            "total 56\n",
            "-rw------- 1 root root 6 Jan 30 08:20 12c6cb72-3ea3-49e7-b381-e0cdfc5e8960.txt\n",
            "-rw------- 1 root root 6 Jan 30 08:20 1e241dc8-8f18-4955-8988-03a0ab49f813.txt\n",
            "-rw------- 1 root root 6 Jan 30 08:20 22e54a62-57a8-4a0a-88c1-4b9758f67651.txt\n",
            "-rw------- 1 root root 7 Jan 30 08:20 316b64c0-55bf-4079-a1c0-d93f461a576f.txt\n",
            "-rw------- 1 root root 6 Jan 30 08:20 37170dd1-2802-4e38-b982-c5d07c64ff67.txt\n",
            "-rw------- 1 root root 6 Jan 30 08:20 4be2025c-09f7-4bb0-b1bd-8e8633e6dec1.txt\n",
            "-rw------- 1 root root 6 Jan 30 08:20 5b562a61-34ad-4f00-9164-d34abb7a38e4.txt\n",
            "-rw------- 1 root root 6 Jan 30 08:20 c9368c55-210d-456c-a5ef-c310e60039ec.txt\n",
            "-rw------- 1 root root 7 Jan 30 08:20 car13.txt\n",
            "-rw------- 1 root root 6 Jan 30 08:20 car14.txt\n",
            "-rw------- 1 root root 5 Jan 30 08:20 car15.txt\n",
            "-rw------- 1 root root 6 Jan 30 08:20 car16.txt\n",
            "-rw------- 1 root root 7 Jan 30 08:20 car17.txt\n",
            "-rw------- 1 root root 6 Jan 30 08:20 car19.txt\n",
            "-rw------- 1 root root 6 Jan 30 08:20 car1.txt\n",
            "-rw------- 1 root root 6 Jan 30 08:20 car20.txt\n",
            "-rw------- 1 root root 6 Jan 30 08:20 car22.txt\n",
            "-rw------- 1 root root 7 Jan 30 08:20 car3.txt\n",
            "-rw------- 1 root root 6 Jan 30 08:20 car5.txt\n",
            "-rw------- 1 root root 7 Jan 30 08:20 car6.txt\n",
            "-rw------- 1 root root 5 Jan 30 08:20 car7.txt\n",
            "-rw------- 1 root root 6 Jan 30 08:20 car8.txt\n",
            "-rw------- 1 root root 6 Jan 30 08:20 car9-0.txt\n",
            "-rw------- 1 root root 7 Jan 30 08:20 car9-1.txt\n",
            "-rw------- 1 root root 7 Jan 30 08:20 car9-2.txt\n",
            "-rw------- 1 root root 7 Jan 30 08:20 car9-4.txt\n",
            "-rw------- 1 root root 7 Jan 30 08:20 car9-5.txt\n",
            "-rw------- 1 root root 7 Jan 30 08:20 car9-7.txt\n",
            "-rw------- 1 root root 7 Jan 30 08:20 car9-9.txt\n",
            "-rw------- 1 root root 6 Jan 30 08:20 car9.txt\n",
            "-rw------- 1 root root 6 Jan 30 08:20 cfaa9dd2-a388-4e92-bb3a-ae65c28d8139.txt\n",
            "-rw------- 1 root root 6 Jan 30 08:20 f8fc5e59-9083-466b-ae3f-6b869a0b257b.txt\n",
            "-rw------- 1 root root 6 Jan 30 08:20 us10.txt\n",
            "-rw------- 1 root root 7 Jan 30 08:20 us1.txt\n",
            "-rw------- 1 root root 6 Jan 30 08:20 us2.txt\n",
            "-rw------- 1 root root 7 Jan 30 08:20 us3.txt\n",
            "-rw------- 1 root root 6 Jan 30 08:20 us4.txt\n",
            "-rw------- 1 root root 8 Jan 30 08:20 us5.txt\n",
            "-rw------- 1 root root 7 Jan 30 08:20 us7.txt\n",
            "-rw------- 1 root root 7 Jan 30 08:20 wts-lg-000021.txt\n",
            "-rw------- 1 root root 6 Jan 30 08:20 wts-lg-000022.txt\n",
            "-rw------- 1 root root 7 Jan 30 08:20 wts-lg-000023.txt\n",
            "-rw------- 1 root root 6 Jan 30 08:20 wts-lg-000025.txt\n",
            "-rw------- 1 root root 6 Jan 30 08:20 wts-lg-000027.txt\n",
            "-rw------- 1 root root 6 Jan 30 08:20 wts-lg-000035.txt\n",
            "-rw------- 1 root root 6 Jan 30 08:20 wts-lg-000036.txt\n",
            "-rw------- 1 root root 5 Jan 30 08:20 wts-lg-000038.txt\n",
            "-rw------- 1 root root 7 Jan 30 08:20 wts-lg-000041.txt\n",
            "-rw------- 1 root root 6 Jan 30 08:20 wts-lg-000042.txt\n",
            "-rw------- 1 root root 6 Jan 30 08:20 wts-lg-000050.txt\n",
            "-rw------- 1 root root 6 Jan 30 08:20 wts-lg-000051.txt\n",
            "-rw------- 1 root root 6 Jan 30 08:20 wts-lg-000055.txt\n",
            "-rw------- 1 root root 6 Jan 30 08:20 wts-lg-000060.txt\n",
            "-rw------- 1 root root 6 Jan 30 08:20 wts-lg-000061.txt\n",
            "-rw------- 1 root root 6 Jan 30 08:20 wts-lg-000062.txt\n",
            "-rw------- 1 root root 6 Jan 30 08:20 wts-lg-000063.txt\n",
            "-rw------- 1 root root 6 Jan 30 08:20 wts-lg-000064.txt\n",
            "-rw------- 1 root root 6 Jan 30 08:20 wts-lg-000072.txt\n",
            "-rw------- 1 root root 7 Jan 30 08:20 wts-lg-000073.txt\n",
            "-rw------- 1 root root 7 Jan 30 08:20 wts-lg-000075.txt\n",
            "-rw------- 1 root root 6 Jan 30 08:20 wts-lg-000079.txt\n",
            "-rw------- 1 root root 6 Jan 30 08:20 wts-lg-000080.txt\n",
            "-rw------- 1 root root 6 Jan 30 08:20 wts-lg-000086.txt\n",
            "-rw------- 1 root root 6 Jan 30 08:20 wts-lg-000087.txt\n",
            "-rw------- 1 root root 6 Jan 30 08:20 wts-lg-000088.txt\n",
            "-rw------- 1 root root 6 Jan 30 08:20 wts-lg-000090.txt\n",
            "-rw------- 1 root root 6 Jan 30 08:20 wts-lg-000091.txt\n",
            "-rw------- 1 root root 6 Jan 30 08:20 wts-lg-000096.txt\n",
            "-rw------- 1 root root 6 Jan 30 08:20 wts-lg-000098.txt\n",
            "-rw------- 1 root root 6 Jan 30 08:20 wts-lg-000099.txt\n",
            "-rw------- 1 root root 6 Jan 30 08:20 wts-lg-000113.txt\n",
            "-rw------- 1 root root 6 Jan 30 08:20 wts-lg-000115.txt\n",
            "-rw------- 1 root root 7 Jan 30 08:20 wts-lg-000119.txt\n",
            "-rw------- 1 root root 7 Jan 30 08:20 wts-lg-000120.txt\n",
            "-rw------- 1 root root 7 Jan 30 08:20 wts-lg-000121.txt\n",
            "-rw------- 1 root root 6 Jan 30 08:20 wts-lg-000122.txt\n",
            "-rw------- 1 root root 7 Jan 30 08:20 wts-lg-000124.txt\n",
            "-rw------- 1 root root 7 Jan 30 08:20 wts-lg-000125.txt\n",
            "-rw------- 1 root root 7 Jan 30 08:20 wts-lg-000127.txt\n",
            "-rw------- 1 root root 7 Jan 30 08:20 wts-lg-000128.txt\n",
            "-rw------- 1 root root 7 Jan 30 08:20 wts-lg-000130.txt\n",
            "-rw------- 1 root root 7 Jan 30 08:20 wts-lg-000131.txt\n",
            "-rw------- 1 root root 7 Jan 30 08:20 wts-lg-000133.txt\n",
            "-rw------- 1 root root 7 Jan 30 08:20 wts-lg-000135.txt\n",
            "-rw------- 1 root root 7 Jan 30 08:20 wts-lg-000137.txt\n",
            "-rw------- 1 root root 7 Jan 30 08:20 wts-lg-000138.txt\n",
            "-rw------- 1 root root 7 Jan 30 08:20 wts-lg-000140.txt\n",
            "-rw------- 1 root root 7 Jan 30 08:20 wts-lg-000141.txt\n",
            "-rw------- 1 root root 7 Jan 30 08:20 wts-lg-000144.txt\n",
            "-rw------- 1 root root 7 Jan 30 08:20 wts-lg-000146.txt\n",
            "-rw------- 1 root root 7 Jan 30 08:20 wts-lg-000147.txt\n",
            "-rw------- 1 root root 7 Jan 30 08:20 wts-lg-000149.txt\n",
            "-rw------- 1 root root 7 Jan 30 08:20 wts-lg-000152.txt\n",
            "-rw------- 1 root root 7 Jan 30 08:20 wts-lg-000154.txt\n",
            "-rw------- 1 root root 7 Jan 30 08:20 wts-lg-000155.txt\n",
            "-rw------- 1 root root 7 Jan 30 08:20 wts-lg-000158.txt\n",
            "-rw------- 1 root root 7 Jan 30 08:20 wts-lg-000164.txt\n",
            "-rw------- 1 root root 7 Jan 30 08:20 wts-lg-000167.txt\n",
            "-rw------- 1 root root 7 Jan 30 08:20 wts-lg-000169.txt\n",
            "-rw------- 1 root root 7 Jan 30 08:20 wts-lg-000172.txt\n",
            "-rw------- 1 root root 7 Jan 30 08:20 wts-lg-000175.txt\n",
            "-rw------- 1 root root 7 Jan 30 08:20 wts-lg-000176.txt\n",
            "-rw------- 1 root root 7 Jan 30 08:20 wts-lg-000178.txt\n",
            "-rw------- 1 root root 6 Jan 30 08:20 wts-lg-000179.txt\n",
            "-rw------- 1 root root 6 Jan 30 08:20 wts-lg-000183.txt\n",
            "-rw------- 1 root root 6 Jan 30 08:20 wts-lg-000186.txt\n",
            "-rw------- 1 root root 6 Jan 30 08:20 wts-lg-000189.txt\n",
            "-rw------- 1 root root 7 Jan 30 08:20 wts-lg-000191.txt\n",
            "-rw------- 1 root root 7 Jan 30 08:20 wts-lg-000192.txt\n",
            "-rw------- 1 root root 7 Jan 30 08:20 wts-lg-000196.txt\n",
            "-rw------- 1 root root 7 Jan 30 08:20 wts-lg-000197.txt\n"
          ]
        }
      ],
      "source": [
        "# verify\n",
        "!echo $DATA_DIR\n",
        "!ls -l $DATA_DIR/\n",
        "!ls -l $DATA_DIR/train\n",
        "!ls -l $DATA_DIR/train/image\n",
        "!ls -l $DATA_DIR/train/label\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2B28ZwpPyox"
      },
      "source": [
        "### 1.1 Download pretrained model from NGC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IU1c75iPPyox"
      },
      "source": [
        "We will use NGC CLI to get the pre-trained models. For more details, go to https://ngc.nvidia.com and click the SETUP on the navigation bar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "85pHGpdOPyoy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12cc45f9-f788-421d-a3f3-eb15470e90f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: LOCAL_PROJECT_DIR=/ngc_content/\n",
            "env: CLI=ngccli_cat_linux.zip\n",
            "--2023-01-30 08:20:41--  https://ngc.nvidia.com/downloads/ngccli_cat_linux.zip\n",
            "Resolving ngc.nvidia.com (ngc.nvidia.com)... 18.65.229.16, 18.65.229.127, 18.65.229.96, ...\n",
            "Connecting to ngc.nvidia.com (ngc.nvidia.com)|18.65.229.16|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 41720199 (40M) [application/zip]\n",
            "Saving to: ‘/ngc_content//ngccli/ngccli_cat_linux.zip’\n",
            "\n",
            "ngccli_cat_linux.zi 100%[===================>]  39.79M  74.3MB/s    in 0.5s    \n",
            "\n",
            "2023-01-30 08:20:41 (74.3 MB/s) - ‘/ngc_content//ngccli/ngccli_cat_linux.zip’ saved [41720199/41720199]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Installing NGC CLI on the local machine.\n",
        "## Download and install\n",
        "%env LOCAL_PROJECT_DIR=/ngc_content/\n",
        "%env CLI=ngccli_cat_linux.zip\n",
        "!sudo mkdir -p $LOCAL_PROJECT_DIR/ngccli && sudo chmod -R 777 $LOCAL_PROJECT_DIR\n",
        "\n",
        "# Remove any previously existing CLI installations\n",
        "!sudo rm -rf $LOCAL_PROJECT_DIR/ngccli/*\n",
        "!wget \"https://ngc.nvidia.com/downloads/$CLI\" -P $LOCAL_PROJECT_DIR/ngccli\n",
        "!unzip -u -q \"$LOCAL_PROJECT_DIR/ngccli/$CLI\" -d $LOCAL_PROJECT_DIR/ngccli/\n",
        "!rm $LOCAL_PROJECT_DIR/ngccli/*.zip \n",
        "os.environ[\"PATH\"]=\"{}/ngccli/ngc-cli:{}\".format(os.getenv(\"LOCAL_PROJECT_DIR\", \"\"), os.getenv(\"PATH\", \"\"))\n",
        "!cp /usr/lib/x86_64-linux-gnu/libstdc++.so.6 $LOCAL_PROJECT_DIR/ngccli/ngc-cli/libstdc++.so.6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "IcsQ_FWqPyoy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a287b0f-2c1e-4eab-adbd-6ced93c094be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+----------+--------+------------+-----------+------------------+-----------+-----------------+--------------+\n",
            "| Version         | Accuracy | Epochs | Batch Size | GPU Model | Memory Footprint | File Size | Status          | Created Date |\n",
            "+-----------------+----------+--------+------------+-----------+------------------+-----------+-----------------+--------------+\n",
            "| trainable_v1.0  | 99.67    | 120    | 1          | V100      | 221.1            | 221.06 MB | UPLOAD_COMPLETE | Aug 24, 2021 |\n",
            "| deployable_v1.0 | 99.67    | 120    | 1          | V100      | 110.1            | 110.09 MB | UPLOAD_COMPLETE | Aug 24, 2021 |\n",
            "+-----------------+----------+--------+------------+-----------+------------------+-----------+-----------------+--------------+\n"
          ]
        }
      ],
      "source": [
        "!ngc registry model list nvidia/tao/lprnet:*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "dAx-bsIDPyoy"
      },
      "outputs": [],
      "source": [
        "!mkdir -p $EXPERIMENT_DIR/pretrained_lprnet_baseline18/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "8EieO4uYPyoy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47cf9db9-d736-4e1c-db7e-c5e725789381"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded 221.13 MB in 10s, Download speed: 22.09 MB/s               \n",
            "--------------------------------------------------------------------------------\n",
            "   Transfer id: lprnet_vtrainable_v1.0\n",
            "   Download status: Completed\n",
            "   Downloaded local path: /content/drive/MyDrive/results/lprnet/pretrained_lprnet_baseline18/lprnet_vtrainable_v1.0\n",
            "   Total files downloaded: 4\n",
            "   Total downloaded size: 221.13 MB\n",
            "   Started at: 2023-01-30 08:20:57.539571\n",
            "   Completed at: 2023-01-30 08:21:07.551506\n",
            "   Duration taken: 10s\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Pull pretrained model from NGC\n",
        "!ngc registry model download-version nvidia/tao/lprnet:trainable_v1.0 --dest $EXPERIMENT_DIR/pretrained_lprnet_baseline18"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "m0ANlm3XPyoz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7dbda42f-99fb-419e-ab36-a7fce1269bae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Check that model is downloaded into dir.\n",
            "total 226365\n",
            "-rw------- 1 root root       200 Jan 30 08:21 ch_lp_characters.txt\n",
            "-rw------- 1 root root 115963904 Jan 30 08:21 ch_lprnet_baseline18_trainable.tlt\n",
            "-rw------- 1 root root        70 Jan 30 08:21 us_lp_characters.txt\n",
            "-rw------- 1 root root 115832832 Jan 30 08:21 us_lprnet_baseline18_trainable.tlt\n"
          ]
        }
      ],
      "source": [
        "print(\"Check that model is downloaded into dir.\")\n",
        "!ls -l $EXPERIMENT_DIR/pretrained_lprnet_baseline18/lprnet_vtrainable_v1.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_26rCobXcri1"
      },
      "source": [
        "## 2. Setup GPU environment <a class=\"anchor\" id=\"head-2\"></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7Cx1_lMded7"
      },
      "source": [
        "### 2.1 Connect to GPU Instance <a class=\"anchor\" id=\"head-2-1\"></a>\n",
        "\n",
        "1. Move any data saved to the Colab Instance storage to Google Drive  \n",
        "2. Change Runtime type to GPU by Runtime(Top Left tab)->Change Runtime Type->GPU(Hardware Accelerator)\n",
        "3.   Then click on Connect (Top Right)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yl8BoM0Jhzh9"
      },
      "source": [
        "### 2.2 Mounting Google drive <a class=\"anchor\" id=\"head-2-2\"></a>\n",
        "Mount your Google drive storage to this Colab instance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Vk2m-N4Nh0Sd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aadbb034-f2ea-42b4-ab3f-9db0029e4616"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: GOOGLE_COLAB=1\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    %env GOOGLE_COLAB=1\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "except:\n",
        "    %env GOOGLE_COLAB=0\n",
        "    print(\"Warning: Not a Colab Environment\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBV_YWiTc_KM"
      },
      "source": [
        "### 2.3 Setup Python environment <a class=\"anchor\" id=\"head-2-3\"></a>\n",
        "Setup the environment necessary to run the TAO Networks by running the bash script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "s2Xygw-y8fjm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "295d2f71-1bad-40bf-da06-61e486a096f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ InRelease [3,622 B]\n",
            "\r0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com (185.1\r0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com (185.1\r                                                                               \rIgn:2 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu2004/x86_64  InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com (185.1\r                                                                               \rHit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease\n",
            "Hit:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu2004/x86_64  Release\n",
            "Get:5 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\n",
            "Get:6 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal InRelease [18.1 kB]\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu focal InRelease\n",
            "Get:9 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\n",
            "Hit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu focal InRelease\n",
            "Get:11 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease [18.1 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\n",
            "Get:13 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 Packages [1,882 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [2,009 kB]\n",
            "Hit:15 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu focal InRelease\n",
            "Get:16 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal/main Sources [2,381 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [2,920 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1,290 kB]\n",
            "Get:19 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [2,442 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [988 kB]\n",
            "Get:21 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal/main amd64 Packages [1,127 kB]\n",
            "Get:22 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal/main amd64 Packages [29.5 kB]\n",
            "Fetched 15.4 MB in 4s (3,770 kB/s)\n",
            "Reading package lists... Done\n",
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ InRelease\n",
            "Ign:2 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu2004/x86_64  InRelease\n",
            "Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease\n",
            "Hit:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu2004/x86_64  Release\n",
            "Hit:5 http://security.ubuntu.com/ubuntu focal-security InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu focal InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu focal-updates InRelease\n",
            "Hit:9 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal InRelease\n",
            "Hit:10 http://archive.ubuntu.com/ubuntu focal-backports InRelease\n",
            "Hit:11 http://ppa.launchpad.net/cran/libgit2/ubuntu focal InRelease\n",
            "Hit:12 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease\n",
            "Hit:13 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu focal InRelease\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libpython3.6-minimal libpython3.6-stdlib python3.6-minimal\n",
            "Suggested packages:\n",
            "  python3.6-venv binfmt-support\n",
            "The following NEW packages will be installed:\n",
            "  libpython3.6-minimal libpython3.6-stdlib python3.6 python3.6-minimal\n",
            "0 upgraded, 4 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 4,294 kB of archives.\n",
            "After this operation, 22.1 MB of additional disk space will be used.\n",
            "Get:1 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal/main amd64 libpython3.6-minimal amd64 3.6.15-1+focal3 [569 kB]\n",
            "Get:2 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal/main amd64 python3.6-minimal amd64 3.6.15-1+focal3 [1,718 kB]\n",
            "Get:3 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal/main amd64 libpython3.6-stdlib amd64 3.6.15-1+focal3 [1,758 kB]\n",
            "Get:4 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal/main amd64 python3.6 amd64 3.6.15-1+focal3 [248 kB]\n",
            "Fetched 4,294 kB in 4s (1,054 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 4.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libpython3.6-minimal:amd64.\n",
            "(Reading database ... 129499 files and directories currently installed.)\n",
            "Preparing to unpack .../libpython3.6-minimal_3.6.15-1+focal3_amd64.deb ...\n",
            "Unpacking libpython3.6-minimal:amd64 (3.6.15-1+focal3) ...\n",
            "Selecting previously unselected package python3.6-minimal.\n",
            "Preparing to unpack .../python3.6-minimal_3.6.15-1+focal3_amd64.deb ...\n",
            "Unpacking python3.6-minimal (3.6.15-1+focal3) ...\n",
            "Selecting previously unselected package libpython3.6-stdlib:amd64.\n",
            "Preparing to unpack .../libpython3.6-stdlib_3.6.15-1+focal3_amd64.deb ...\n",
            "Unpacking libpython3.6-stdlib:amd64 (3.6.15-1+focal3) ...\n",
            "Selecting previously unselected package python3.6.\n",
            "Preparing to unpack .../python3.6_3.6.15-1+focal3_amd64.deb ...\n",
            "Unpacking python3.6 (3.6.15-1+focal3) ...\n",
            "Setting up libpython3.6-minimal:amd64 (3.6.15-1+focal3) ...\n",
            "Setting up python3.6-minimal (3.6.15-1+focal3) ...\n",
            "Setting up libpython3.6-stdlib:amd64 (3.6.15-1+focal3) ...\n",
            "Setting up python3.6 (3.6.15-1+focal3) ...\n",
            "Processing triggers for man-db (2.9.1-1) ...\n",
            "Processing triggers for mime-support (3.64ubuntu1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  python-pip-whl python3-setuptools python3-wheel\n",
            "Suggested packages:\n",
            "  python-setuptools-doc\n",
            "The following NEW packages will be installed:\n",
            "  python-pip-whl python3-pip python3-setuptools python3-wheel\n",
            "0 upgraded, 4 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 2,389 kB of archives.\n",
            "After this operation, 4,933 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 python-pip-whl all 20.0.2-5ubuntu1.7 [1,805 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 python3-setuptools all 45.2.0-1ubuntu0.1 [330 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 python3-wheel all 0.34.2-1ubuntu0.1 [23.9 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 python3-pip all 20.0.2-5ubuntu1.7 [230 kB]\n",
            "Fetched 2,389 kB in 1s (1,653 kB/s)\n",
            "Selecting previously unselected package python-pip-whl.\n",
            "(Reading database ... 130110 files and directories currently installed.)\n",
            "Preparing to unpack .../python-pip-whl_20.0.2-5ubuntu1.7_all.deb ...\n",
            "Unpacking python-pip-whl (20.0.2-5ubuntu1.7) ...\n",
            "Selecting previously unselected package python3-setuptools.\n",
            "Preparing to unpack .../python3-setuptools_45.2.0-1ubuntu0.1_all.deb ...\n",
            "Unpacking python3-setuptools (45.2.0-1ubuntu0.1) ...\n",
            "Selecting previously unselected package python3-wheel.\n",
            "Preparing to unpack .../python3-wheel_0.34.2-1ubuntu0.1_all.deb ...\n",
            "Unpacking python3-wheel (0.34.2-1ubuntu0.1) ...\n",
            "Selecting previously unselected package python3-pip.\n",
            "Preparing to unpack .../python3-pip_20.0.2-5ubuntu1.7_all.deb ...\n",
            "Unpacking python3-pip (20.0.2-5ubuntu1.7) ...\n",
            "Setting up python3-setuptools (45.2.0-1ubuntu0.1) ...\n",
            "Setting up python3-wheel (0.34.2-1ubuntu0.1) ...\n",
            "Setting up python-pip-whl (20.0.2-5ubuntu1.7) ...\n",
            "Setting up python3-pip (20.0.2-5ubuntu1.7) ...\n",
            "Processing triggers for man-db (2.9.1-1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  python3.6-lib2to3\n",
            "The following NEW packages will be installed:\n",
            "  python3.6-distutils python3.6-lib2to3\n",
            "0 upgraded, 2 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 308 kB of archives.\n",
            "After this operation, 1,232 kB of additional disk space will be used.\n",
            "Get:1 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal/main amd64 python3.6-lib2to3 all 3.6.15-1+focal3 [122 kB]\n",
            "Get:2 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal/main amd64 python3.6-distutils all 3.6.15-1+focal3 [187 kB]\n",
            "Fetched 308 kB in 1s (227 kB/s)\n",
            "Selecting previously unselected package python3.6-lib2to3.\n",
            "(Reading database ... 130474 files and directories currently installed.)\n",
            "Preparing to unpack .../python3.6-lib2to3_3.6.15-1+focal3_all.deb ...\n",
            "Unpacking python3.6-lib2to3 (3.6.15-1+focal3) ...\n",
            "Selecting previously unselected package python3.6-distutils.\n",
            "Preparing to unpack .../python3.6-distutils_3.6.15-1+focal3_all.deb ...\n",
            "Unpacking python3.6-distutils (3.6.15-1+focal3) ...\n",
            "Setting up python3.6-lib2to3 (3.6.15-1+focal3) ...\n",
            "Setting up python3.6-distutils (3.6.15-1+focal3) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libpython3.6 libpython3.6-dev\n",
            "The following NEW packages will be installed:\n",
            "  libpython3.6 libpython3.6-dev python3.6-dev\n",
            "0 upgraded, 3 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 5,329 kB of archives.\n",
            "After this operation, 22.1 MB of additional disk space will be used.\n",
            "Get:1 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal/main amd64 libpython3.6 amd64 3.6.15-1+focal3 [1,396 kB]\n",
            "Get:2 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal/main amd64 libpython3.6-dev amd64 3.6.15-1+focal3 [3,433 kB]\n",
            "Get:3 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal/main amd64 python3.6-dev amd64 3.6.15-1+focal3 [501 kB]\n",
            "Fetched 5,329 kB in 3s (1,576 kB/s)\n",
            "Selecting previously unselected package libpython3.6:amd64.\n",
            "(Reading database ... 130613 files and directories currently installed.)\n",
            "Preparing to unpack .../libpython3.6_3.6.15-1+focal3_amd64.deb ...\n",
            "Unpacking libpython3.6:amd64 (3.6.15-1+focal3) ...\n",
            "Selecting previously unselected package libpython3.6-dev:amd64.\n",
            "Preparing to unpack .../libpython3.6-dev_3.6.15-1+focal3_amd64.deb ...\n",
            "Unpacking libpython3.6-dev:amd64 (3.6.15-1+focal3) ...\n",
            "Selecting previously unselected package python3.6-dev.\n",
            "Preparing to unpack .../python3.6-dev_3.6.15-1+focal3_amd64.deb ...\n",
            "Unpacking python3.6-dev (3.6.15-1+focal3) ...\n",
            "Setting up libpython3.6:amd64 (3.6.15-1+focal3) ...\n",
            "Setting up libpython3.6-dev:amd64 (3.6.15-1+focal3) ...\n",
            "Setting up python3.6-dev (3.6.15-1+focal3) ...\n",
            "Processing triggers for man-db (2.9.1-1) ...\n",
            "rm: cannot remove '/usr/bin/python': No such file or directory\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pip\n",
            "  Downloading pip-21.3.1-py3-none-any.whl (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 7.5 MB/s \n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 20.0.2\n",
            "    Not uninstalling pip at /usr/lib/python3/dist-packages, outside environment /usr\n",
            "    Can't uninstall 'pip'. No files were found to uninstall.\n",
            "Successfully installed pip-21.3.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting google-colab\n",
            "  Downloading google-colab-1.0.0.tar.gz (72 kB)\n",
            "     |████████████████████████████████| 72 kB 1.5 MB/s             \n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting google-auth~=1.4.0\n",
            "  Downloading google_auth-1.4.2-py2.py3-none-any.whl (64 kB)\n",
            "     |████████████████████████████████| 64 kB 3.1 MB/s             \n",
            "\u001b[?25hCollecting ipykernel~=4.6.0\n",
            "  Downloading ipykernel-4.6.1-py3-none-any.whl (104 kB)\n",
            "     |████████████████████████████████| 104 kB 62.0 MB/s            \n",
            "\u001b[?25hCollecting ipython~=5.5.0\n",
            "  Downloading ipython-5.5.0-py3-none-any.whl (758 kB)\n",
            "     |████████████████████████████████| 758 kB 55.7 MB/s            \n",
            "\u001b[?25hCollecting notebook~=5.2.0\n",
            "  Downloading notebook-5.2.2-py2.py3-none-any.whl (8.0 MB)\n",
            "     |████████████████████████████████| 8.0 MB 59.3 MB/s            \n",
            "\u001b[?25hCollecting pandas~=0.24.0\n",
            "  Downloading pandas-0.24.2-cp36-cp36m-manylinux1_x86_64.whl (10.1 MB)\n",
            "     |████████████████████████████████| 10.1 MB 71.2 MB/s            \n",
            "\u001b[?25hCollecting portpicker~=1.2.0\n",
            "  Downloading portpicker-1.2.0.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting requests~=2.21.0\n",
            "  Downloading requests-2.21.0-py2.py3-none-any.whl (57 kB)\n",
            "     |████████████████████████████████| 57 kB 6.3 MB/s             \n",
            "\u001b[?25hCollecting six~=1.12.0\n",
            "  Downloading six-1.12.0-py2.py3-none-any.whl (10 kB)\n",
            "Collecting tornado~=4.5.0\n",
            "  Downloading tornado-4.5.3.tar.gz (484 kB)\n",
            "     |████████████████████████████████| 484 kB 71.9 MB/s            \n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting rsa>=3.1.4\n",
            "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
            "Collecting pyasn1-modules>=0.2.1\n",
            "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
            "     |████████████████████████████████| 155 kB 74.6 MB/s            \n",
            "\u001b[?25hCollecting cachetools>=2.0.0\n",
            "  Downloading cachetools-4.2.4-py3-none-any.whl (10 kB)\n",
            "Collecting traitlets>=4.1.0\n",
            "  Downloading traitlets-4.3.3-py2.py3-none-any.whl (75 kB)\n",
            "     |████████████████████████████████| 75 kB 4.9 MB/s             \n",
            "\u001b[?25hCollecting jupyter-client\n",
            "  Downloading jupyter_client-7.1.2-py3-none-any.whl (130 kB)\n",
            "     |████████████████████████████████| 130 kB 76.2 MB/s            \n",
            "\u001b[?25hCollecting pexpect\n",
            "  Downloading pexpect-4.8.0-py2.py3-none-any.whl (59 kB)\n",
            "     |████████████████████████████████| 59 kB 7.7 MB/s             \n",
            "\u001b[?25hCollecting simplegeneric>0.8\n",
            "  Downloading simplegeneric-0.8.1.zip (12 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting decorator\n",
            "  Downloading decorator-5.1.1-py3-none-any.whl (9.1 kB)\n",
            "Collecting prompt-toolkit<2.0.0,>=1.0.4\n",
            "  Downloading prompt_toolkit-1.0.18-py3-none-any.whl (245 kB)\n",
            "     |████████████████████████████████| 245 kB 68.1 MB/s            \n",
            "\u001b[?25hCollecting pickleshare\n",
            "  Downloading pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/lib/python3/dist-packages (from ipython~=5.5.0->google-colab) (45.2.0)\n",
            "Collecting pygments\n",
            "  Downloading Pygments-2.14.0-py3-none-any.whl (1.1 MB)\n",
            "     |████████████████████████████████| 1.1 MB 66.3 MB/s            \n",
            "\u001b[?25hCollecting nbconvert\n",
            "  Downloading nbconvert-6.0.7-py3-none-any.whl (552 kB)\n",
            "     |████████████████████████████████| 552 kB 77.3 MB/s            \n",
            "\u001b[?25hCollecting jupyter-core\n",
            "  Downloading jupyter_core-4.9.2-py3-none-any.whl (86 kB)\n",
            "     |████████████████████████████████| 86 kB 7.2 MB/s             \n",
            "\u001b[?25hCollecting jinja2\n",
            "  Downloading Jinja2-3.0.3-py3-none-any.whl (133 kB)\n",
            "     |████████████████████████████████| 133 kB 76.2 MB/s            \n",
            "\u001b[?25hCollecting terminado>=0.3.3\n",
            "  Downloading terminado-0.12.1-py3-none-any.whl (15 kB)\n",
            "Collecting nbformat\n",
            "  Downloading nbformat-5.1.3-py3-none-any.whl (178 kB)\n",
            "     |████████████████████████████████| 178 kB 77.4 MB/s            \n",
            "\u001b[?25hCollecting ipython-genutils\n",
            "  Downloading ipython_genutils-0.2.0-py2.py3-none-any.whl (26 kB)\n",
            "Collecting numpy>=1.12.0\n",
            "  Downloading numpy-1.19.5-cp36-cp36m-manylinux2010_x86_64.whl (14.8 MB)\n",
            "     |████████████████████████████████| 14.8 MB 54.0 MB/s            \n",
            "\u001b[?25hCollecting pytz>=2011k\n",
            "  Downloading pytz-2022.7.1-py2.py3-none-any.whl (499 kB)\n",
            "     |████████████████████████████████| 499 kB 78.0 MB/s            \n",
            "\u001b[?25hCollecting python-dateutil>=2.5.0\n",
            "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
            "     |████████████████████████████████| 247 kB 77.9 MB/s            \n",
            "\u001b[?25hCollecting urllib3<1.25,>=1.21.1\n",
            "  Downloading urllib3-1.24.3-py2.py3-none-any.whl (118 kB)\n",
            "     |████████████████████████████████| 118 kB 70.9 MB/s            \n",
            "\u001b[?25hCollecting chardet<3.1.0,>=3.0.2\n",
            "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
            "     |████████████████████████████████| 133 kB 74.1 MB/s            \n",
            "\u001b[?25hCollecting idna<2.9,>=2.5\n",
            "  Downloading idna-2.8-py2.py3-none-any.whl (58 kB)\n",
            "     |████████████████████████████████| 58 kB 7.8 MB/s             \n",
            "\u001b[?25hCollecting certifi>=2017.4.17\n",
            "  Downloading certifi-2022.12.7-py3-none-any.whl (155 kB)\n",
            "     |████████████████████████████████| 155 kB 77.3 MB/s            \n",
            "\u001b[?25hCollecting wcwidth\n",
            "  Downloading wcwidth-0.2.6-py2.py3-none-any.whl (29 kB)\n",
            "Collecting pyasn1<0.5.0,>=0.4.6\n",
            "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
            "     |████████████████████████████████| 77 kB 7.5 MB/s             \n",
            "\u001b[?25hCollecting ptyprocess\n",
            "  Downloading ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB)\n",
            "Collecting MarkupSafe>=2.0\n",
            "  Downloading MarkupSafe-2.0.1-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (30 kB)\n",
            "Collecting pyzmq>=13\n",
            "  Downloading pyzmq-25.0.0-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
            "     |████████████████████████████████| 1.0 MB 73.1 MB/s            \n",
            "\u001b[?25hCollecting nest-asyncio>=1.5\n",
            "  Downloading nest_asyncio-1.5.6-py3-none-any.whl (5.2 kB)\n",
            "Collecting entrypoints\n",
            "  Downloading entrypoints-0.4-py3-none-any.whl (5.3 kB)\n",
            "Collecting pandocfilters>=1.4.1\n",
            "  Downloading pandocfilters-1.5.0-py2.py3-none-any.whl (8.7 kB)\n",
            "Collecting bleach\n",
            "  Downloading bleach-4.1.0-py2.py3-none-any.whl (157 kB)\n",
            "     |████████████████████████████████| 157 kB 63.2 MB/s            \n",
            "\u001b[?25hCollecting mistune<2,>=0.8.1\n",
            "  Downloading mistune-0.8.4-py2.py3-none-any.whl (16 kB)\n",
            "Collecting defusedxml\n",
            "  Downloading defusedxml-0.7.1-py2.py3-none-any.whl (25 kB)\n",
            "Collecting jupyterlab-pygments\n",
            "  Downloading jupyterlab_pygments-0.1.2-py2.py3-none-any.whl (4.6 kB)\n",
            "Collecting testpath\n",
            "  Downloading testpath-0.6.0-py3-none-any.whl (83 kB)\n",
            "     |████████████████████████████████| 83 kB 3.5 MB/s             \n",
            "\u001b[?25hCollecting nbclient<0.6.0,>=0.5.0\n",
            "  Downloading nbclient-0.5.9-py3-none-any.whl (69 kB)\n",
            "     |████████████████████████████████| 69 kB 9.8 MB/s             \n",
            "\u001b[?25hCollecting jsonschema!=2.5.0,>=2.4\n",
            "  Downloading jsonschema-3.2.0-py2.py3-none-any.whl (56 kB)\n",
            "     |████████████████████████████████| 56 kB 4.5 MB/s             \n",
            "\u001b[?25hCollecting importlib-metadata\n",
            "  Downloading importlib_metadata-4.8.3-py3-none-any.whl (17 kB)\n",
            "Collecting pyrsistent>=0.14.0\n",
            "  Downloading pyrsistent-0.18.0-cp36-cp36m-manylinux1_x86_64.whl (117 kB)\n",
            "     |████████████████████████████████| 117 kB 75.0 MB/s            \n",
            "\u001b[?25hCollecting attrs>=17.4.0\n",
            "  Downloading attrs-22.2.0-py3-none-any.whl (60 kB)\n",
            "     |████████████████████████████████| 60 kB 9.0 MB/s             \n",
            "\u001b[?25hCollecting async-generator\n",
            "  Downloading async_generator-1.10-py3-none-any.whl (18 kB)\n",
            "Collecting packaging\n",
            "  Downloading packaging-21.3-py3-none-any.whl (40 kB)\n",
            "     |████████████████████████████████| 40 kB 6.7 MB/s             \n",
            "\u001b[?25hCollecting webencodings\n",
            "  Downloading webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
            "Collecting zipp>=0.5\n",
            "  Downloading zipp-3.6.0-py3-none-any.whl (5.3 kB)\n",
            "Collecting typing-extensions>=3.6.4\n",
            "  Downloading typing_extensions-4.1.1-py3-none-any.whl (26 kB)\n",
            "Collecting pyparsing!=3.0.5,>=2.0.2\n",
            "  Downloading pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
            "     |████████████████████████████████| 98 kB 9.5 MB/s             \n",
            "\u001b[?25hBuilding wheels for collected packages: google-colab, portpicker, tornado, simplegeneric\n",
            "  Building wheel for google-colab (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for google-colab: filename=google_colab-1.0.0-py2.py3-none-any.whl size=102274 sha256=c57101086d11a5813d293f2d0fe3620fa174a948731c3a2ab72f2bbb576e0358\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/8e/4e/4ded5bcfbef742f58343bedd6e993868ddf3772f8d1e7c02c8\n",
            "  Building wheel for portpicker (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for portpicker: filename=portpicker-1.2.0-py3-none-any.whl size=13369 sha256=9b617e1d4ce3ef8e7fb0b35c609f19167921718fa8e8324b88e53daa1d90c0e1\n",
            "  Stored in directory: /root/.cache/pip/wheels/c7/4c/3b/04310724e2522e9f078d0b2c54e844ea0541ef8f631d5a011a\n",
            "  Building wheel for tornado (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tornado: filename=tornado-4.5.3-cp36-cp36m-linux_x86_64.whl size=433519 sha256=7e3af5a8c2e0df83720665b3027a1e03ce58f9511391f8284a205f24f2baca9a\n",
            "  Stored in directory: /root/.cache/pip/wheels/7c/26/a6/8fc2a26547829568bcafabbabe61ea5f9c449f97be752af46a\n",
            "  Building wheel for simplegeneric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for simplegeneric: filename=simplegeneric-0.8.1-py3-none-any.whl size=5073 sha256=a1355fbf13554971d7d142c19a7cd11c251e3e18fc57fc6842060a21997b2f8b\n",
            "  Stored in directory: /root/.cache/pip/wheels/aa/2e/cc/c837f9b4f1927c7bded8233b73a186b2d303daf0af7ecb07f6\n",
            "Successfully built google-colab portpicker tornado simplegeneric\n",
            "Installing collected packages: zipp, typing-extensions, six, ipython-genutils, decorator, traitlets, pyrsistent, importlib-metadata, attrs, wcwidth, tornado, pyzmq, python-dateutil, pyparsing, ptyprocess, nest-asyncio, jupyter-core, jsonschema, entrypoints, webencodings, simplegeneric, pygments, prompt-toolkit, pickleshare, pexpect, packaging, nbformat, MarkupSafe, jupyter-client, async-generator, testpath, pyasn1, pandocfilters, nbclient, mistune, jupyterlab-pygments, jinja2, ipython, defusedxml, bleach, urllib3, terminado, rsa, pytz, pyasn1-modules, numpy, nbconvert, ipykernel, idna, chardet, certifi, cachetools, requests, portpicker, pandas, notebook, google-auth, google-colab\n",
            "Successfully installed MarkupSafe-2.0.1 async-generator-1.10 attrs-22.2.0 bleach-4.1.0 cachetools-4.2.4 certifi-2022.12.7 chardet-3.0.4 decorator-5.1.1 defusedxml-0.7.1 entrypoints-0.4 google-auth-1.4.2 google-colab-1.0.0 idna-2.8 importlib-metadata-4.8.3 ipykernel-4.6.1 ipython-5.5.0 ipython-genutils-0.2.0 jinja2-3.0.3 jsonschema-3.2.0 jupyter-client-7.1.2 jupyter-core-4.9.2 jupyterlab-pygments-0.1.2 mistune-0.8.4 nbclient-0.5.9 nbconvert-6.0.7 nbformat-5.1.3 nest-asyncio-1.5.6 notebook-5.2.2 numpy-1.19.5 packaging-21.3 pandas-0.24.2 pandocfilters-1.5.0 pexpect-4.8.0 pickleshare-0.7.5 portpicker-1.2.0 prompt-toolkit-1.0.18 ptyprocess-0.7.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 pygments-2.14.0 pyparsing-3.0.9 pyrsistent-0.18.0 python-dateutil-2.8.2 pytz-2022.7.1 pyzmq-25.0.0 requests-2.21.0 rsa-4.9 simplegeneric-0.8.1 six-1.12.0 terminado-0.12.1 testpath-0.6.0 tornado-4.5.3 traitlets-4.3.3 typing-extensions-4.1.1 urllib3-1.24.3 wcwidth-0.2.6 webencodings-0.5.1 zipp-3.6.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting nvidia-pyindex\n",
            "  Downloading nvidia-pyindex-1.0.9.tar.gz (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: nvidia-pyindex\n",
            "  Building wheel for nvidia-pyindex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nvidia-pyindex: filename=nvidia_pyindex-1.0.9-py3-none-any.whl size=8398 sha256=6a128b2e7bfcd19f56f7797f56d4ccc509f4ba2c3b13f5afc12d662b4d3e45e1\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/79/65/9cb980b5f481843cd9896e1579abc1c1f608b5f9e60ca90e03\n",
            "Successfully built nvidia-pyindex\n",
            "Installing collected packages: nvidia-pyindex\n",
            "Successfully installed nvidia-pyindex-1.0.9\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/, https://pypi.ngc.nvidia.com\n",
            "Collecting nvidia-horovod==0.20.0+nv20.10\n",
            "  Downloading https://developer.download.nvidia.com/compute/redist/nvidia-horovod/nvidia_horovod-0.20.0+nv20.10-cp36-cp36m-linux_x86_64.whl (17.7 MB)\n",
            "     |████████████████████████████████| 17.7 MB 7.6 MB/s            \n",
            "\u001b[?25hCollecting psutil\n",
            "  Downloading psutil-5.9.4-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (280 kB)\n",
            "     |████████████████████████████████| 280 kB 7.5 MB/s            \n",
            "\u001b[?25hCollecting cloudpickle\n",
            "  Downloading cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
            "Collecting pyyaml\n",
            "  Downloading PyYAML-6.0-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (603 kB)\n",
            "     |████████████████████████████████| 603 kB 75.1 MB/s            \n",
            "\u001b[?25hCollecting dataclasses\n",
            "  Downloading dataclasses-0.8-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: pyyaml, psutil, dataclasses, cloudpickle, nvidia-horovod\n",
            "Successfully installed cloudpickle-2.2.1 dataclasses-0.8 nvidia-horovod-0.20.0+nv20.10 psutil-5.9.4 pyyaml-6.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/, https://pypi.ngc.nvidia.com\n",
            "Collecting nvidia-tensorflow==1.15.4+nv20.10\n",
            "  Downloading https://developer.download.nvidia.com/compute/redist/nvidia-tensorflow/nvidia_tensorflow-1.15.4+nv20.10-cp36-cp36m-linux_x86_64.whl (492.4 MB)\n",
            "     |████████████████████████████████| 492.4 MB 11 kB/s             \n",
            "\u001b[?25hCollecting nvidia-tensorboard<1.16.0,>=1.15.0\n",
            "  Downloading https://developer.download.nvidia.com/compute/redist/nvidia-tensorboard/nvidia_tensorboard-1.15.0%2Bnv21.04-py3-none-any.whl (3.8 MB)\n",
            "     |████████████████████████████████| 3.8 MB 45.8 MB/s            \n",
            "\u001b[?25hCollecting wrapt>=1.11.1\n",
            "  Downloading wrapt-1.14.1-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (74 kB)\n",
            "     |████████████████████████████████| 74 kB 2.9 MB/s            \n",
            "\u001b[?25hCollecting keras-preprocessing>=1.0.5\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "     |████████████████████████████████| 42 kB 1.5 MB/s             \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/lib/python3/dist-packages (from nvidia-tensorflow==1.15.4+nv20.10) (0.34.2)\n",
            "Collecting nvidia-tensorboard<1.16.0,>=1.15.0\n",
            "  Downloading https://developer.download.nvidia.com/compute/redist/nvidia-tensorboard/nvidia_tensorboard-1.15.0%2Bnv20.10-py3-none-any.whl (3.8 MB)\n",
            "     |████████████████████████████████| 3.8 MB 56.9 MB/s            \n",
            "\u001b[?25hCollecting nvidia-cuda-nvcc==11.1.74\n",
            "  Downloading https://developer.download.nvidia.com/compute/redist/nvidia-cuda-nvcc/nvidia_cuda_nvcc-11.1.74-cp36-cp36m-linux_x86_64.whl (11.7 MB)\n",
            "     |████████████████████████████████| 11.7 MB 57.2 MB/s            \n",
            "\u001b[?25hCollecting nvidia-cusolver==11.0.0.74\n",
            "  Downloading https://developer.download.nvidia.com/compute/redist/nvidia-cusolver/nvidia_cusolver-11.0.0.74-cp36-cp36m-linux_x86_64.whl (612.1 MB)\n",
            "     |████████████████████████████████| 612.1 MB 15 kB/s             \n",
            "\u001b[?25hCollecting google-pasta>=0.1.6\n",
            "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "     |████████████████████████████████| 57 kB 4.7 MB/s             \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from nvidia-tensorflow==1.15.4+nv20.10) (1.12.0)\n",
            "Collecting termcolor>=1.1.0\n",
            "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting nvidia-nccl==2.7.8\n",
            "  Downloading https://developer.download.nvidia.com/compute/redist/nvidia-nccl/nvidia_nccl-2.7.8-cp36-cp36m-linux_x86_64.whl (86.3 MB)\n",
            "     |████████████████████████████████| 86.3 MB 1.2 MB/s             \n",
            "\u001b[?25hCollecting astor>=0.6.0\n",
            "  Downloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
            "Collecting nvidia-cuda-cupti==11.1.69\n",
            "  Downloading https://developer.download.nvidia.com/compute/redist/nvidia-cuda-cupti/nvidia_cuda_cupti-11.1.69-cp36-cp36m-linux_x86_64.whl (8.2 MB)\n",
            "     |████████████████████████████████| 8.2 MB 53.6 MB/s            \n",
            "\u001b[?25hCollecting nvidia-cufft==10.3.0.74\n",
            "  Downloading https://developer.download.nvidia.com/compute/redist/nvidia-cufft/nvidia_cufft-10.3.0.74-cp36-cp36m-linux_x86_64.whl (168.4 MB)\n",
            "     |████████████████████████████████| 168.4 MB 82 kB/s              \n",
            "\u001b[?25hCollecting nvidia-curand==10.2.2.74\n",
            "  Downloading https://developer.download.nvidia.com/compute/redist/nvidia-curand/nvidia_curand-10.2.2.74-cp36-cp36m-linux_x86_64.whl (48.4 MB)\n",
            "     |████████████████████████████████| 48.4 MB 1.2 MB/s             \n",
            "\u001b[?25hCollecting absl-py>=0.7.0\n",
            "  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
            "     |████████████████████████████████| 126 kB 43.4 MB/s            \n",
            "\u001b[?25hCollecting numpy<1.19.0,>=1.16.0\n",
            "  Downloading numpy-1.18.5-cp36-cp36m-manylinux1_x86_64.whl (20.1 MB)\n",
            "     |████████████████████████████████| 20.1 MB 1.1 MB/s             \n",
            "\u001b[?25hCollecting nvidia-cusparse==11.2.0.275\n",
            "  Downloading https://developer.download.nvidia.com/compute/redist/nvidia-cusparse/nvidia_cusparse-11.2.0.275-cp36-cp36m-linux_x86_64.whl (159.5 MB)\n",
            "     |████████████████████████████████| 159.5 MB 14 kB/s              \n",
            "\u001b[?25hCollecting nvidia-tensorrt==7.2.1.4\n",
            "  Downloading https://developer.download.nvidia.com/compute/redist/nvidia-tensorrt/nvidia_tensorrt-7.2.1.4-cp36-none-linux_x86_64.whl (263.8 MB)\n",
            "     |████████████████████████████████| 263.8 MB 30 kB/s             \n",
            "\u001b[?25hCollecting keras-applications>=1.0.8\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "     |████████████████████████████████| 50 kB 7.7 MB/s             \n",
            "\u001b[?25hCollecting nvidia-cudnn==8.0.4.30\n",
            "  Downloading https://developer.download.nvidia.com/compute/redist/nvidia-cudnn/nvidia_cudnn-8.0.4.30-cp36-cp36m-linux_x86_64.whl (847.5 MB)\n",
            "     |███████████████████████████████▌| 834.1 MB 1.2 MB/s eta 0:00:12  tcmalloc: large alloc 1147494400 bytes == 0x7f154000 @  0x7fbe17204680 0x7fbe17224da2 0x59da29 0x5eb77c 0x4dabfd 0x50e870 0x5110dd 0x50fae2 0x50e15d 0x50ecb6 0x5110dd 0x50fae2 0x50e15d 0x50ecb6 0x511fbc 0x59beb7 0x511573 0x59beb7 0x511573 0x59beb7 0x511573 0x50d3c9 0x59f271 0x5ad083 0x5444f3 0x5ac06c 0x50ede5 0x5110dd 0x50fae2 0x50e15d 0x50ecb6\n",
            "     |████████████████████████████████| 847.5 MB 18 kB/s               \n",
            "\u001b[?25hCollecting protobuf>=3.6.1\n",
            "  Downloading protobuf-3.19.6-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "     |████████████████████████████████| 1.1 MB 61.1 MB/s            \n",
            "\u001b[?25hCollecting grpcio>=1.8.6\n",
            "  Downloading grpcio-1.48.2-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
            "     |████████████████████████████████| 4.6 MB 68.7 MB/s            \n",
            "\u001b[?25hCollecting nvidia-cuda-runtime==11.1.74\n",
            "  Downloading https://developer.download.nvidia.com/compute/redist/nvidia-cuda-runtime/nvidia_cuda_runtime-11.1.74-cp36-cp36m-linux_x86_64.whl (2.5 MB)\n",
            "     |████████████████████████████████| 2.5 MB 49.1 MB/s            \n",
            "\u001b[?25hCollecting gast==0.2.2\n",
            "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "  Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n",
            "     |████████████████████████████████| 503 kB 69.2 MB/s            \n",
            "\u001b[?25hCollecting opt-einsum>=2.3.2\n",
            "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
            "     |████████████████████████████████| 65 kB 3.8 MB/s             \n",
            "\u001b[?25hCollecting nvidia-dali-nvtf-plugin==0.26.0+nv20.10\n",
            "  Downloading https://developer.download.nvidia.com/compute/redist/nvidia-dali-nvtf-plugin/nvidia_dali_nvtf_plugin-0.26.0%2Bnv20.10-cp36-cp36m-linux_x86_64.whl (65 kB)\n",
            "     |████████████████████████████████| 65 kB 4.8 MB/s             \n",
            "\u001b[?25hCollecting nvidia-cublas==11.2.1.74\n",
            "  Downloading https://developer.download.nvidia.com/compute/redist/nvidia-cublas/nvidia_cublas-11.2.1.74-cp36-cp36m-linux_x86_64.whl (231.4 MB)\n",
            "     |████████████████████████████████| 231.4 MB 27 kB/s              \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from nvidia-cublas==11.2.1.74->nvidia-tensorflow==1.15.4+nv20.10) (45.2.0)\n",
            "Collecting nvidia-dali-cuda110==0.26.0\n",
            "  Downloading https://developer.download.nvidia.com/compute/redist/nvidia-dali-cuda110/nvidia_dali_cuda110-0.26.0-1608709-py3-none-manylinux2014_x86_64.whl (365.4 MB)\n",
            "     |████████████████████████████████| 365.4 MB 37 kB/s             \n",
            "\u001b[?25hRequirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from nvidia-tensorboard<1.16.0,>=1.15.0->nvidia-tensorflow==1.15.4+nv20.10) (0.5.1)\n",
            "Collecting markdown>=2.6.8\n",
            "  Downloading Markdown-3.3.7-py3-none-any.whl (97 kB)\n",
            "     |████████████████████████████████| 97 kB 8.3 MB/s             \n",
            "\u001b[?25hCollecting werkzeug>=0.11.15\n",
            "  Downloading Werkzeug-2.0.3-py3-none-any.whl (289 kB)\n",
            "     |████████████████████████████████| 289 kB 82.1 MB/s            \n",
            "\u001b[?25hCollecting nvidia-cuda-nvrtc<11.2,>=11.1\n",
            "  Downloading https://developer.download.nvidia.com/compute/redist/nvidia-cuda-nvrtc/nvidia_cuda_nvrtc-11.1.105-py3-none-manylinux1_x86_64.whl (15.0 MB)\n",
            "     |████████████████████████████████| 15.0 MB 51.2 MB/s            \n",
            "\u001b[?25hCollecting h5py\n",
            "  Downloading h5py-3.1.0-cp36-cp36m-manylinux1_x86_64.whl (4.0 MB)\n",
            "     |████████████████████████████████| 4.0 MB 71.1 MB/s            \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->nvidia-tensorboard<1.16.0,>=1.15.0->nvidia-tensorflow==1.15.4+nv20.10) (4.8.3)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from werkzeug>=0.11.15->nvidia-tensorboard<1.16.0,>=1.15.0->nvidia-tensorflow==1.15.4+nv20.10) (0.8)\n",
            "Collecting cached-property\n",
            "  Downloading cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->nvidia-tensorboard<1.16.0,>=1.15.0->nvidia-tensorflow==1.15.4+nv20.10) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->nvidia-tensorboard<1.16.0,>=1.15.0->nvidia-tensorflow==1.15.4+nv20.10) (4.1.1)\n",
            "Building wheels for collected packages: gast, termcolor\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7539 sha256=14ed31939907e585c0b043c97a53a64032596292b3bb78f81324d886c4637ddc\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/a7/b9/0740c7a3a7d1d348f04823339274b90de25fbcd217b2ee1fbe\n",
            "  Building wheel for termcolor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4830 sha256=de7abb00985008b0cbadf08242399fcea8003954445afa18bf84fdad0893bbc8\n",
            "  Stored in directory: /root/.cache/pip/wheels/93/2a/eb/e58dbcbc963549ee4f065ff80a59f274cc7210b6eab962acdc\n",
            "Successfully built gast termcolor\n",
            "Installing collected packages: numpy, cached-property, werkzeug, protobuf, nvidia-dali-cuda110, nvidia-cudnn, nvidia-cuda-runtime, nvidia-cuda-nvrtc, nvidia-cublas, markdown, h5py, grpcio, absl-py, wrapt, termcolor, tensorflow-estimator, opt-einsum, nvidia-tensorrt, nvidia-tensorboard, nvidia-nccl, nvidia-dali-nvtf-plugin, nvidia-cusparse, nvidia-cusolver, nvidia-curand, nvidia-cufft, nvidia-cuda-nvcc, nvidia-cuda-cupti, keras-preprocessing, keras-applications, google-pasta, gast, astor, nvidia-tensorflow\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "Successfully installed absl-py-1.4.0 astor-0.8.1 cached-property-1.5.2 gast-0.2.2 google-pasta-0.2.0 grpcio-1.48.2 h5py-3.1.0 keras-applications-1.0.8 keras-preprocessing-1.1.2 markdown-3.3.7 numpy-1.18.5 nvidia-cublas-11.2.1.74 nvidia-cuda-cupti-11.1.69 nvidia-cuda-nvcc-11.1.74 nvidia-cuda-nvrtc-11.1.105 nvidia-cuda-runtime-11.1.74 nvidia-cudnn-8.0.4.30 nvidia-cufft-10.3.0.74 nvidia-curand-10.2.2.74 nvidia-cusolver-11.0.0.74 nvidia-cusparse-11.2.0.275 nvidia-dali-cuda110-0.26.0 nvidia-dali-nvtf-plugin-0.26.0+nv20.10 nvidia-nccl-2.7.8 nvidia-tensorboard-1.15.0+nv20.10 nvidia-tensorflow-1.15.4+nv20.10 nvidia-tensorrt-7.2.1.4 opt-einsum-3.3.0 protobuf-3.19.6 tensorflow-estimator-1.15.1 termcolor-1.1.0 werkzeug-2.0.3 wrapt-1.14.1\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "--2023-01-30 08:29:01--  https://github.com/Kitware/CMake/releases/download/v3.14.4/cmake-3.14.4-Linux-x86_64.sh\n",
            "Resolving github.com (github.com)... 192.30.255.113\n",
            "Connecting to github.com (github.com)|192.30.255.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/537699/fc11d880-7650-11e9-969f-7442127f007a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230130%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230130T082901Z&X-Amz-Expires=300&X-Amz-Signature=2a755b0687919d339c6128b82902b9dd6a9881f570e8848694f6c44dbbbe6eb5&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=537699&response-content-disposition=attachment%3B%20filename%3Dcmake-3.14.4-Linux-x86_64.sh&response-content-type=application%2Foctet-stream [following]\n",
            "--2023-01-30 08:29:01--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/537699/fc11d880-7650-11e9-969f-7442127f007a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230130%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230130T082901Z&X-Amz-Expires=300&X-Amz-Signature=2a755b0687919d339c6128b82902b9dd6a9881f570e8848694f6c44dbbbe6eb5&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=537699&response-content-disposition=attachment%3B%20filename%3Dcmake-3.14.4-Linux-x86_64.sh&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 37196929 (35M) [application/octet-stream]\n",
            "Saving to: ‘cmake-3.14.4-Linux-x86_64.sh’\n",
            "\n",
            "cmake-3.14.4-Linux- 100%[===================>]  35.47M  17.6MB/s    in 2.0s    \n",
            "\n",
            "2023-01-30 08:29:04 (17.6 MB/s) - ‘cmake-3.14.4-Linux-x86_64.sh’ saved [37196929/37196929]\n",
            "\n",
            "CMake Installer Version: 3.14.4, Copyright (c) Kitware\n",
            "This is a self-extracting archive.\n",
            "The archive will be extracted to: /usr/local\n",
            "\n",
            "Using target directory: /usr/local\n",
            "Extracting, please wait...\n",
            "\n",
            "Unpacking finished successfully\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/, https://pypi.ngc.nvidia.com\n",
            "Collecting nvidia-eff==0.5.3\n",
            "  Downloading https://developer.download.nvidia.com/compute/redist/nvidia-eff/nvidia_eff-0.5.3-cp36-cp36m-linux_x86_64.whl (739 kB)\n",
            "     |████████████████████████████████| 739 kB 7.2 MB/s            \n",
            "\u001b[?25hCollecting black==19.10b0\n",
            "  Downloading black-19.10b0-py36-none-any.whl (97 kB)\n",
            "     |████████████████████████████████| 97 kB 4.5 MB/s            \n",
            "\u001b[?25hCollecting pybind11>=2.6\n",
            "  Downloading pybind11-2.10.3-py3-none-any.whl (222 kB)\n",
            "     |████████████████████████████████| 222 kB 41.5 MB/s            \n",
            "\u001b[?25hRequirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from nvidia-eff==0.5.3) (0.8)\n",
            "Collecting pyinstaller\n",
            "  Downloading pyinstaller-4.10-py3-none-manylinux2014_x86_64.whl (1.5 MB)\n",
            "     |████████████████████████████████| 1.5 MB 70.3 MB/s            \n",
            "\u001b[?25hCollecting isort[requirements]<5\n",
            "  Downloading isort-4.3.21-py2.py3-none-any.whl (42 kB)\n",
            "     |████████████████████████████████| 42 kB 857 kB/s             \n",
            "\u001b[?25hCollecting pyarmor\n",
            "  Downloading pyarmor-7.7.4-py2.py3-none-any.whl (2.3 MB)\n",
            "     |████████████████████████████████| 2.3 MB 63.6 MB/s            \n",
            "\u001b[?25hCollecting ruamel.yaml\n",
            "  Downloading ruamel.yaml-0.17.21-py3-none-any.whl (109 kB)\n",
            "     |████████████████████████████████| 109 kB 77.4 MB/s            \n",
            "\u001b[?25hCollecting cryptography\n",
            "  Downloading cryptography-39.0.0-cp36-abi3-manylinux_2_28_x86_64.whl (4.2 MB)\n",
            "     |████████████████████████████████| 4.2 MB 68.1 MB/s            \n",
            "\u001b[?25hRequirement already satisfied: wrapt in /usr/local/lib/python3.6/dist-packages (from nvidia-eff==0.5.3) (1.14.1)\n",
            "Collecting appdirs\n",
            "  Using cached appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.6/dist-packages (from black==19.10b0->nvidia-eff==0.5.3) (22.2.0)\n",
            "Collecting typed-ast>=1.4.0\n",
            "  Downloading typed_ast-1.5.4-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (831 kB)\n",
            "     |████████████████████████████████| 831 kB 72.8 MB/s            \n",
            "\u001b[?25hCollecting click>=6.5\n",
            "  Downloading click-8.0.4-py3-none-any.whl (97 kB)\n",
            "     |████████████████████████████████| 97 kB 8.2 MB/s             \n",
            "\u001b[?25hCollecting toml>=0.9.4\n",
            "  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
            "Collecting regex\n",
            "  Downloading regex-2022.10.31-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (756 kB)\n",
            "     |████████████████████████████████| 756 kB 74.1 MB/s            \n",
            "\u001b[?25hCollecting pathspec<1,>=0.6\n",
            "  Downloading pathspec-0.9.0-py2.py3-none-any.whl (31 kB)\n",
            "Collecting pip-api\n",
            "  Downloading pip_api-0.0.26-py3-none-any.whl (110 kB)\n",
            "     |████████████████████████████████| 110 kB 75.0 MB/s            \n",
            "\u001b[?25hCollecting pipreqs\n",
            "  Downloading pipreqs-0.4.11-py2.py3-none-any.whl (32 kB)\n",
            "Collecting cffi>=1.12\n",
            "  Downloading cffi-1.15.1-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (402 kB)\n",
            "     |████████████████████████████████| 402 kB 75.4 MB/s            \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.6/dist-packages (from pyinstaller->nvidia-eff==0.5.3) (4.8.3)\n",
            "Collecting pyinstaller-hooks-contrib>=2020.6\n",
            "  Downloading pyinstaller_hooks_contrib-2022.0-py2.py3-none-any.whl (222 kB)\n",
            "     |████████████████████████████████| 222 kB 73.4 MB/s            \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from pyinstaller->nvidia-eff==0.5.3) (45.2.0)\n",
            "Collecting altgraph\n",
            "  Downloading altgraph-0.17.3-py2.py3-none-any.whl (21 kB)\n",
            "Collecting ruamel.yaml.clib>=0.2.6\n",
            "  Downloading ruamel.yaml.clib-0.2.7-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (503 kB)\n",
            "     |████████████████████████████████| 503 kB 70.0 MB/s            \n",
            "\u001b[?25hCollecting pycparser\n",
            "  Downloading pycparser-2.21-py2.py3-none-any.whl (118 kB)\n",
            "     |████████████████████████████████| 118 kB 65.2 MB/s            \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata->pyinstaller->nvidia-eff==0.5.3) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata->pyinstaller->nvidia-eff==0.5.3) (3.6.0)\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.6/dist-packages (from pip-api->isort[requirements]<5->nvidia-eff==0.5.3) (21.3.1)\n",
            "Collecting yarg\n",
            "  Downloading yarg-0.1.9-py2.py3-none-any.whl (19 kB)\n",
            "Collecting docopt\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from yarg->pipreqs->isort[requirements]<5->nvidia-eff==0.5.3) (2.21.0)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->yarg->pipreqs->isort[requirements]<5->nvidia-eff==0.5.3) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->yarg->pipreqs->isort[requirements]<5->nvidia-eff==0.5.3) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->yarg->pipreqs->isort[requirements]<5->nvidia-eff==0.5.3) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->yarg->pipreqs->isort[requirements]<5->nvidia-eff==0.5.3) (2022.12.7)\n",
            "Building wheels for collected packages: docopt\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13704 sha256=6ee87e704644c8fef05aa1eb403a48ce64d05c730fda3b06a49d056ce6897aaa\n",
            "  Stored in directory: /root/.cache/pip/wheels/3f/2a/fa/4d7a888e69774d5e6e855d190a8a51b357d77cc05eb1c097c9\n",
            "Successfully built docopt\n",
            "Installing collected packages: yarg, pycparser, docopt, typed-ast, toml, ruamel.yaml.clib, regex, pyinstaller-hooks-contrib, pipreqs, pip-api, pathspec, isort, click, cffi, appdirs, altgraph, ruamel.yaml, pyinstaller, pybind11, pyarmor, cryptography, black, nvidia-eff\n",
            "Successfully installed altgraph-0.17.3 appdirs-1.4.4 black-19.10b0 cffi-1.15.1 click-8.0.4 cryptography-39.0.0 docopt-0.6.2 isort-4.3.21 nvidia-eff-0.5.3 pathspec-0.9.0 pip-api-0.0.26 pipreqs-0.4.11 pyarmor-7.7.4 pybind11-2.10.3 pycparser-2.21 pyinstaller-4.10 pyinstaller-hooks-contrib-2022.0 regex-2022.10.31 ruamel.yaml-0.17.21 ruamel.yaml.clib-0.2.7 toml-0.10.2 typed-ast-1.5.4 yarg-0.1.9\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/, https://pypi.ngc.nvidia.com\n",
            "Collecting nvidia-tao==4.0.0\n",
            "  Downloading nvidia_tao-4.0.0-py3-none-any.whl (155 kB)\n",
            "     |████████████████████████████████| 155 kB 6.7 MB/s            \n",
            "\u001b[?25hCollecting docker==4.3.1\n",
            "  Downloading docker-4.3.1-py2.py3-none-any.whl (145 kB)\n",
            "     |████████████████████████████████| 145 kB 53.1 MB/s            \n",
            "\u001b[?25hRequirement already satisfied: chardet==3.0.4 in /usr/local/lib/python3.6/dist-packages (from nvidia-tao==4.0.0) (3.0.4)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from nvidia-tao==4.0.0) (2022.12.7)\n",
            "Collecting six==1.15.0\n",
            "  Downloading six-1.15.0-py2.py3-none-any.whl (10 kB)\n",
            "Collecting tabulate==0.8.7\n",
            "  Downloading tabulate-0.8.7-py3-none-any.whl (24 kB)\n",
            "Collecting urllib3>=1.26.5\n",
            "  Downloading urllib3-1.26.14-py2.py3-none-any.whl (140 kB)\n",
            "     |████████████████████████████████| 140 kB 75.4 MB/s            \n",
            "\u001b[?25hCollecting websocket-client==0.57.0\n",
            "  Downloading websocket_client-0.57.0-py2.py3-none-any.whl (200 kB)\n",
            "     |████████████████████████████████| 200 kB 47.5 MB/s            \n",
            "\u001b[?25hCollecting docker-pycreds==0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting idna==2.10\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "     |████████████████████████████████| 58 kB 7.7 MB/s             \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from nvidia-tao==4.0.0) (2.21.0)\n",
            "Collecting requests\n",
            "  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
            "     |████████████████████████████████| 63 kB 2.1 MB/s             \n",
            "\u001b[?25hCollecting charset-normalizer~=2.0.0\n",
            "  Downloading charset_normalizer-2.0.12-py3-none-any.whl (39 kB)\n",
            "Installing collected packages: urllib3, six, idna, charset-normalizer, websocket-client, requests, tabulate, docker-pycreds, docker, nvidia-tao\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.12.0\n",
            "    Uninstalling six-1.12.0:\n",
            "      Successfully uninstalled six-1.12.0\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 2.8\n",
            "    Uninstalling idna-2.8:\n",
            "      Successfully uninstalled idna-2.8\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.21.0\n",
            "    Uninstalling requests-2.21.0:\n",
            "      Successfully uninstalled requests-2.21.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests~=2.21.0, but you have requests 2.27.1 which is incompatible.\n",
            "google-colab 1.0.0 requires six~=1.12.0, but you have six 1.15.0 which is incompatible.\u001b[0m\n",
            "Successfully installed charset-normalizer-2.0.12 docker-4.3.1 docker-pycreds-0.4.0 idna-2.10 nvidia-tao-4.0.0 requests-2.27.1 six-1.15.0 tabulate-0.8.7 urllib3-1.26.14 websocket-client-0.57.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/, https://pypi.ngc.nvidia.com, https://developer.download.nvidia.com/compute/redist\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting git+https://github.com/NVIDIA/dllogger.git (from -r /content/drive/MyDrive/nvidia-tao/tensorflow/requirements-pip.txt (line 56))\n",
            "  Cloning https://github.com/NVIDIA/dllogger.git to ./pip-req-build-rzaymsy8\n",
            "  Running command git clone --filter=blob:none -q https://github.com/NVIDIA/dllogger.git /tmp/pip-req-build-rzaymsy8\n",
            "  Resolved https://github.com/NVIDIA/dllogger.git to commit 0540a43971f4a8a16693a9de9de73c1072020769\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting PyYAML\n",
            "  Using cached PyYAML-6.0-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (603 kB)\n",
            "Collecting addict==2.1.0\n",
            "  Downloading addict-2.1.0.tar.gz (2.2 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting argcomplete==1.9.4\n",
            "  Downloading argcomplete-1.9.4-py2.py3-none-any.whl (36 kB)\n",
            "Collecting argparse==1.4.0\n",
            "  Downloading argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
            "Collecting boto3==1.9.13\n",
            "  Downloading boto3-1.9.13-py2.py3-none-any.whl (128 kB)\n",
            "     |████████████████████████████████| 128 kB 16.7 MB/s            \n",
            "\u001b[?25hCollecting clearml==1.7.1rc1\n",
            "  Downloading clearml-1.7.1rc1-py2.py3-none-any.whl (945 kB)\n",
            "     |████████████████████████████████| 945 kB 70.0 MB/s            \n",
            "\u001b[?25hCollecting cryptography==3.4.8\n",
            "  Downloading cryptography-3.4.8-cp36-abi3-manylinux_2_24_x86_64.whl (3.0 MB)\n",
            "     |████████████████████████████████| 3.0 MB 49.6 MB/s            \n",
            "\u001b[?25hCollecting future==0.17.1\n",
            "  Downloading future-0.17.1.tar.gz (829 kB)\n",
            "     |████████████████████████████████| 829 kB 76.7 MB/s            \n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting graphsurgeon\n",
            "  Downloading https://developer.download.nvidia.com/compute/redist/graphsurgeon/graphsurgeon-0.4.5-py2.py3-none-any.whl (21 kB)\n",
            "Collecting grpcio==1.26.0\n",
            "  Downloading grpcio-1.26.0-cp36-cp36m-manylinux2010_x86_64.whl (2.4 MB)\n",
            "     |████████████████████████████████| 2.4 MB 72.9 MB/s            \n",
            "\u001b[?25hCollecting h5py==2.8.0\n",
            "  Downloading h5py-2.8.0-cp36-cp36m-manylinux1_x86_64.whl (2.8 MB)\n",
            "     |████████████████████████████████| 2.8 MB 72.9 MB/s            \n",
            "\u001b[?25hCollecting jupyter==1.0.0\n",
            "  Downloading jupyter-1.0.0-py2.py3-none-any.whl (2.7 kB)\n",
            "Collecting keras==2.2.4\n",
            "  Downloading Keras-2.2.4-py2.py3-none-any.whl (312 kB)\n",
            "     |████████████████████████████████| 312 kB 76.7 MB/s            \n",
            "\u001b[?25hCollecting keras_metrics==1.1.0\n",
            "  Downloading keras_metrics-1.1.0-py2.py3-none-any.whl (5.6 kB)\n",
            "Collecting keras2onnx==1.7.0\n",
            "  Downloading keras2onnx-1.7.0-py3-none-any.whl (96 kB)\n",
            "     |████████████████████████████████| 96 kB 7.2 MB/s             \n",
            "\u001b[?25hCollecting llvmlite==0.32.0\n",
            "  Downloading llvmlite-0.32.0-cp36-cp36m-manylinux1_x86_64.whl (20.2 MB)\n",
            "     |████████████████████████████████| 20.2 MB 1.2 MB/s             \n",
            "\u001b[?25hCollecting mpi4py==3.0.3\n",
            "  Downloading mpi4py-3.0.3.tar.gz (1.4 MB)\n",
            "     |████████████████████████████████| 1.4 MB 70.6 MB/s            \n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting numba==0.47\n",
            "  Downloading numba-0.47.0-cp36-cp36m-manylinux1_x86_64.whl (3.7 MB)\n",
            "     |████████████████████████████████| 3.7 MB 75.5 MB/s            \n",
            "\u001b[?25hCollecting numpy==1.19.4\n",
            "  Downloading numpy-1.19.4-cp36-cp36m-manylinux2010_x86_64.whl (14.5 MB)\n",
            "     |████████████████████████████████| 14.5 MB 69.7 MB/s            \n",
            "\u001b[?25hCollecting nvidia-ml-py\n",
            "  Downloading nvidia_ml_py-11.525.84-py3-none-any.whl (34 kB)\n",
            "Collecting onnx==1.8.1\n",
            "  Downloading onnx-1.8.1-cp36-cp36m-manylinux2010_x86_64.whl (14.5 MB)\n",
            "     |████████████████████████████████| 14.5 MB 64.2 MB/s            \n",
            "\u001b[?25hCollecting onnxruntime==1.8.0\n",
            "  Downloading onnxruntime-1.8.0-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
            "     |████████████████████████████████| 4.5 MB 65.8 MB/s            \n",
            "\u001b[?25hCollecting onnx_graphsurgeon\n",
            "  Downloading https://developer.download.nvidia.com/compute/redist/onnx-graphsurgeon/onnx_graphsurgeon-0.3.25-py2.py3-none-any.whl (40 kB)\n",
            "     |████████████████████████████████| 40 kB 7.2 MB/s             \n",
            "\u001b[?25hCollecting opencv-python==4.2.0.32\n",
            "  Downloading opencv_python-4.2.0.32-cp36-cp36m-manylinux1_x86_64.whl (28.2 MB)\n",
            "     |████████████████████████████████| 28.2 MB 1.3 MB/s             \n",
            "\u001b[?25hCollecting pandas==0.25.3\n",
            "  Downloading pandas-0.25.3-cp36-cp36m-manylinux1_x86_64.whl (10.4 MB)\n",
            "     |████████████████████████████████| 10.4 MB 60.1 MB/s            \n",
            "\u001b[?25hCollecting posix_ipc==1.0.4\n",
            "  Downloading posix_ipc-1.0.4.tar.gz (78 kB)\n",
            "     |████████████████████████████████| 78 kB 8.5 MB/s             \n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting prettytable==0.7.2\n",
            "  Downloading prettytable-0.7.2.zip (28 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pyarrow==0.11.1\n",
            "  Downloading pyarrow-0.11.1-cp36-cp36m-manylinux1_x86_64.whl (11.6 MB)\n",
            "     |████████████████████████████████| 11.6 MB 76.9 MB/s            \n",
            "\u001b[?25hCollecting pycocotools-fix==2.0.0.9\n",
            "  Downloading pycocotools-fix-2.0.0.9.tar.gz (124 kB)\n",
            "     |████████████████████████████████| 124 kB 79.7 MB/s            \n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pycuda==2020.1\n",
            "  Downloading pycuda-2020.1.tar.gz (1.6 MB)\n",
            "     |████████████████████████████████| 1.6 MB 68.8 MB/s            \n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pyjwt\n",
            "  Downloading PyJWT-2.4.0-py3-none-any.whl (18 kB)\n",
            "Collecting PyYAML\n",
            "  Downloading PyYAML-5.4-cp36-cp36m-manylinux1_x86_64.whl (640 kB)\n",
            "     |████████████████████████████████| 640 kB 58.8 MB/s            \n",
            "\u001b[?25hCollecting recordclass==0.13.1\n",
            "  Downloading recordclass-0.13.1.tar.gz (152 kB)\n",
            "     |████████████████████████████████| 152 kB 73.5 MB/s            \n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting requests==2.20.1\n",
            "  Downloading requests-2.20.1-py2.py3-none-any.whl (57 kB)\n",
            "     |████████████████████████████████| 57 kB 4.8 MB/s             \n",
            "\u001b[?25hCollecting requests_toolbelt==0.8.0\n",
            "  Downloading requests_toolbelt-0.8.0-py2.py3-none-any.whl (54 kB)\n",
            "     |████████████████████████████████| 54 kB 2.5 MB/s             \n",
            "\u001b[?25hCollecting retrying==1.3.3\n",
            "  Downloading retrying-1.3.3.tar.gz (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting seaborn==0.7.1\n",
            "  Downloading seaborn-0.7.1.tar.gz (158 kB)\n",
            "     |████████████████████████████████| 158 kB 66.7 MB/s            \n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting setuptools\n",
            "  Downloading setuptools-59.6.0-py3-none-any.whl (952 kB)\n",
            "     |████████████████████████████████| 952 kB 67.9 MB/s            \n",
            "\u001b[?25hCollecting scikit-image==0.17.2\n",
            "  Downloading scikit_image-0.17.2-cp36-cp36m-manylinux1_x86_64.whl (12.4 MB)\n",
            "     |████████████████████████████████| 12.4 MB 69.1 MB/s            \n",
            "\u001b[?25hCollecting scikit-learn==0.24.2\n",
            "  Downloading scikit_learn-0.24.2-cp36-cp36m-manylinux2010_x86_64.whl (22.2 MB)\n",
            "     |████████████████████████████████| 22.2 MB 1.2 MB/s             \n",
            "\u001b[?25hCollecting scipy==1.5.4\n",
            "  Downloading scipy-1.5.4-cp36-cp36m-manylinux1_x86_64.whl (25.9 MB)\n",
            "     |████████████████████████████████| 25.9 MB 101.8 MB/s            \n",
            "\u001b[?25hCollecting semver==2.7.9\n",
            "  Downloading semver-2.7.9.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting shapely\n",
            "  Downloading Shapely-1.8.5.post1-cp36-cp36m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.0 MB)\n",
            "     |████████████████████████████████| 2.0 MB 62.9 MB/s            \n",
            "\u001b[?25hCollecting simplejson==3.13.2\n",
            "  Downloading simplejson-3.13.2.tar.gz (79 kB)\n",
            "     |████████████████████████████████| 79 kB 9.9 MB/s             \n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting six==1.13.0\n",
            "  Downloading six-1.13.0-py2.py3-none-any.whl (10 kB)\n",
            "Collecting tabulate==0.7.5\n",
            "  Downloading tabulate-0.7.5.tar.gz (29 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tf2onnx==1.9.2\n",
            "  Downloading tf2onnx-1.9.2-py3-none-any.whl (430 kB)\n",
            "     |████████████████████████████████| 430 kB 77.7 MB/s            \n",
            "\u001b[?25hCollecting toposort==1.5\n",
            "  Downloading toposort-1.5-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting tqdm==4.19.5\n",
            "  Downloading tqdm-4.19.5-py2.py3-none-any.whl (51 kB)\n",
            "     |████████████████████████████████| 51 kB 149 kB/s             \n",
            "\u001b[?25hCollecting uplink==0.5.5\n",
            "  Downloading uplink-0.5.5-py2.py3-none-any.whl (40 kB)\n",
            "     |████████████████████████████████| 40 kB 6.1 MB/s             \n",
            "\u001b[?25hCollecting uff\n",
            "  Downloading https://developer.download.nvidia.com/compute/redist/uff/uff-0.6.9-py2.py3-none-any.whl (52 kB)\n",
            "     |████████████████████████████████| 52 kB 1.5 MB/s             \n",
            "\u001b[?25hCollecting urllib3<1.25\n",
            "  Using cached urllib3-1.24.3-py2.py3-none-any.whl (118 kB)\n",
            "Collecting xmltodict==0.12.0\n",
            "  Downloading xmltodict-0.12.0-py2.py3-none-any.whl (9.2 kB)\n",
            "Collecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
            "Collecting s3transfer<0.2.0,>=0.1.10\n",
            "  Downloading s3transfer-0.1.13-py2.py3-none-any.whl (59 kB)\n",
            "     |████████████████████████████████| 59 kB 8.6 MB/s             \n",
            "\u001b[?25hCollecting botocore<1.13.0,>=1.12.13\n",
            "  Downloading botocore-1.12.253-py2.py3-none-any.whl (5.7 MB)\n",
            "     |████████████████████████████████| 5.7 MB 72.4 MB/s            \n",
            "\u001b[?25hCollecting attrs>=18.0\n",
            "  Using cached attrs-22.2.0-py3-none-any.whl (60 kB)\n",
            "Collecting jsonschema>=2.6.0\n",
            "  Using cached jsonschema-3.2.0-py2.py3-none-any.whl (56 kB)\n",
            "Collecting furl>=2.0.0\n",
            "  Downloading furl-2.1.3-py2.py3-none-any.whl (20 kB)\n",
            "Collecting pyparsing>=2.0.3\n",
            "  Using cached pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
            "Collecting python-dateutil>=2.6.1\n",
            "  Using cached python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
            "Collecting pathlib2>=2.3.0\n",
            "  Downloading pathlib2-2.3.7.post1-py2.py3-none-any.whl (18 kB)\n",
            "Collecting Pillow>=4.1.1\n",
            "  Using cached Pillow-8.4.0-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "Collecting psutil>=3.4.2\n",
            "  Using cached psutil-5.9.4-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (280 kB)\n",
            "Collecting cffi>=1.12\n",
            "  Using cached cffi-1.15.1-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (402 kB)\n",
            "Collecting jupyter-console\n",
            "  Downloading jupyter_console-6.4.3-py3-none-any.whl (22 kB)\n",
            "Collecting notebook\n",
            "  Downloading notebook-6.4.10-py3-none-any.whl (9.9 MB)\n",
            "     |████████████████████████████████| 9.9 MB 54.8 MB/s            \n",
            "\u001b[?25hCollecting nbconvert\n",
            "  Using cached nbconvert-6.0.7-py3-none-any.whl (552 kB)\n",
            "Collecting ipywidgets\n",
            "  Downloading ipywidgets-7.7.2-py2.py3-none-any.whl (123 kB)\n",
            "     |████████████████████████████████| 123 kB 79.1 MB/s            \n",
            "\u001b[?25hCollecting qtconsole\n",
            "  Downloading qtconsole-5.2.2-py3-none-any.whl (120 kB)\n",
            "     |████████████████████████████████| 120 kB 76.1 MB/s            \n",
            "\u001b[?25hCollecting ipykernel\n",
            "  Downloading ipykernel-5.5.6-py3-none-any.whl (121 kB)\n",
            "     |████████████████████████████████| 121 kB 78.3 MB/s            \n",
            "\u001b[?25hCollecting keras-preprocessing>=1.0.5\n",
            "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "Collecting keras-applications>=1.0.6\n",
            "  Using cached Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "Collecting onnxconverter-common>=1.7.0\n",
            "  Downloading onnxconverter_common-1.13.0-py2.py3-none-any.whl (83 kB)\n",
            "     |████████████████████████████████| 83 kB 3.0 MB/s             \n",
            "\u001b[?25hCollecting protobuf\n",
            "  Using cached protobuf-3.19.6-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "Collecting fire\n",
            "  Downloading fire-0.5.0.tar.gz (88 kB)\n",
            "     |████████████████████████████████| 88 kB 8.9 MB/s             \n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting typing-extensions>=3.6.2.1\n",
            "  Using cached typing_extensions-4.1.1-py3-none-any.whl (26 kB)\n",
            "Collecting flatbuffers\n",
            "  Downloading flatbuffers-23.1.21-py2.py3-none-any.whl (26 kB)\n",
            "Collecting pytz>=2017.2\n",
            "  Using cached pytz-2022.7.1-py2.py3-none-any.whl (499 kB)\n",
            "Collecting cython>=0.27.3\n",
            "  Using cached Cython-0.29.33-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (2.0 MB)\n",
            "Collecting matplotlib>=2.1.0\n",
            "  Using cached matplotlib-3.3.4-cp36-cp36m-manylinux1_x86_64.whl (11.5 MB)\n",
            "Collecting appdirs>=1.4.0\n",
            "  Using cached appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting decorator>=3.2.0\n",
            "  Using cached decorator-5.1.1-py3-none-any.whl (9.1 kB)\n",
            "Collecting mako\n",
            "  Downloading Mako-1.1.6-py2.py3-none-any.whl (75 kB)\n",
            "     |████████████████████████████████| 75 kB 5.0 MB/s             \n",
            "\u001b[?25hCollecting pytools>=2011.2\n",
            "  Downloading pytools-2022.1.12.tar.gz (70 kB)\n",
            "     |████████████████████████████████| 70 kB 9.5 MB/s             \n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting certifi>=2017.4.17\n",
            "  Using cached certifi-2022.12.7-py3-none-any.whl (155 kB)\n",
            "Collecting idna<2.8,>=2.5\n",
            "  Downloading idna-2.7-py2.py3-none-any.whl (58 kB)\n",
            "     |████████████████████████████████| 58 kB 6.6 MB/s             \n",
            "\u001b[?25hCollecting chardet<3.1.0,>=3.0.2\n",
            "  Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
            "Collecting networkx>=2.0\n",
            "  Downloading networkx-2.5.1-py3-none-any.whl (1.6 MB)\n",
            "     |████████████████████████████████| 1.6 MB 59.2 MB/s            \n",
            "\u001b[?25hCollecting tifffile>=2019.7.26\n",
            "  Downloading tifffile-2020.9.3-py3-none-any.whl (148 kB)\n",
            "     |████████████████████████████████| 148 kB 69.8 MB/s            \n",
            "\u001b[?25hCollecting PyWavelets>=1.1.1\n",
            "  Downloading PyWavelets-1.1.1-cp36-cp36m-manylinux1_x86_64.whl (4.4 MB)\n",
            "     |████████████████████████████████| 4.4 MB 67.7 MB/s            \n",
            "\u001b[?25hCollecting imageio>=2.3.0\n",
            "  Downloading imageio-2.15.0-py3-none-any.whl (3.3 MB)\n",
            "     |████████████████████████████████| 3.3 MB 72.9 MB/s            \n",
            "\u001b[?25hCollecting joblib>=0.11\n",
            "  Downloading joblib-1.1.1-py2.py3-none-any.whl (309 kB)\n",
            "     |████████████████████████████████| 309 kB 73.5 MB/s            \n",
            "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
            "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
            "Collecting flatbuffers\n",
            "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
            "Collecting uritemplate>=3.0.0\n",
            "  Downloading uritemplate-4.1.1-py2.py3-none-any.whl (10 kB)\n",
            "Collecting docutils<0.16,>=0.10\n",
            "  Downloading docutils-0.15.2-py3-none-any.whl (547 kB)\n",
            "     |████████████████████████████████| 547 kB 75.1 MB/s            \n",
            "\u001b[?25hCollecting pycparser\n",
            "  Using cached pycparser-2.21-py2.py3-none-any.whl (118 kB)\n",
            "Collecting orderedmultidict>=1.0.1\n",
            "  Downloading orderedmultidict-1.0.1-py2.py3-none-any.whl (11 kB)\n",
            "Collecting pyrsistent>=0.14.0\n",
            "  Using cached pyrsistent-0.18.0-cp36-cp36m-manylinux1_x86_64.whl (117 kB)\n",
            "Collecting importlib-metadata\n",
            "  Using cached importlib_metadata-4.8.3-py3-none-any.whl (17 kB)\n",
            "Collecting cycler>=0.10\n",
            "  Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
            "Collecting kiwisolver>=1.0.1\n",
            "  Using cached kiwisolver-1.3.1-cp36-cp36m-manylinux1_x86_64.whl (1.1 MB)\n",
            "Collecting decorator>=3.2.0\n",
            "  Downloading decorator-4.4.2-py2.py3-none-any.whl (9.2 kB)\n",
            "Collecting packaging\n",
            "  Using cached packaging-21.3-py3-none-any.whl (40 kB)\n",
            "Collecting platformdirs>=2.2.0\n",
            "  Downloading platformdirs-2.4.0-py3-none-any.whl (14 kB)\n",
            "Collecting dataclasses>=0.7\n",
            "  Using cached dataclasses-0.8-py3-none-any.whl (19 kB)\n",
            "Collecting termcolor\n",
            "  Using cached termcolor-1.1.0-py3-none-any.whl\n",
            "Collecting traitlets>=4.1.0\n",
            "  Using cached traitlets-4.3.3-py2.py3-none-any.whl (75 kB)\n",
            "Collecting jupyter-client\n",
            "  Using cached jupyter_client-7.1.2-py3-none-any.whl (130 kB)\n",
            "Collecting tornado>=4.2\n",
            "  Downloading tornado-6.1-cp36-cp36m-manylinux2010_x86_64.whl (427 kB)\n",
            "     |████████████████████████████████| 427 kB 74.7 MB/s            \n",
            "\u001b[?25hCollecting ipython>=5.0.0\n",
            "  Downloading ipython-7.16.3-py3-none-any.whl (783 kB)\n",
            "     |████████████████████████████████| 783 kB 75.2 MB/s            \n",
            "\u001b[?25hCollecting ipython-genutils\n",
            "  Using cached ipython_genutils-0.2.0-py2.py3-none-any.whl (26 kB)\n",
            "Collecting widgetsnbextension~=3.6.0\n",
            "  Downloading widgetsnbextension-3.6.1-py2.py3-none-any.whl (1.6 MB)\n",
            "     |████████████████████████████████| 1.6 MB 60.5 MB/s            \n",
            "\u001b[?25hCollecting jupyterlab-widgets<3,>=1.0.0\n",
            "  Downloading jupyterlab_widgets-1.1.1-py3-none-any.whl (245 kB)\n",
            "     |████████████████████████████████| 245 kB 76.7 MB/s            \n",
            "\u001b[?25hCollecting pygments\n",
            "  Using cached Pygments-2.14.0-py3-none-any.whl (1.1 MB)\n",
            "Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0\n",
            "  Downloading prompt_toolkit-3.0.36-py3-none-any.whl (386 kB)\n",
            "     |████████████████████████████████| 386 kB 69.4 MB/s            \n",
            "\u001b[?25hCollecting MarkupSafe>=0.9.2\n",
            "  Using cached MarkupSafe-2.0.1-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (30 kB)\n",
            "Collecting defusedxml\n",
            "  Using cached defusedxml-0.7.1-py2.py3-none-any.whl (25 kB)\n",
            "Collecting testpath\n",
            "  Using cached testpath-0.6.0-py3-none-any.whl (83 kB)\n",
            "Collecting jupyter-core\n",
            "  Using cached jupyter_core-4.9.2-py3-none-any.whl (86 kB)\n",
            "Collecting jupyterlab-pygments\n",
            "  Using cached jupyterlab_pygments-0.1.2-py2.py3-none-any.whl (4.6 kB)\n",
            "Collecting bleach\n",
            "  Using cached bleach-4.1.0-py2.py3-none-any.whl (157 kB)\n",
            "Collecting jinja2>=2.4\n",
            "  Using cached Jinja2-3.0.3-py3-none-any.whl (133 kB)\n",
            "Collecting pandocfilters>=1.4.1\n",
            "  Using cached pandocfilters-1.5.0-py2.py3-none-any.whl (8.7 kB)\n",
            "Collecting mistune<2,>=0.8.1\n",
            "  Using cached mistune-0.8.4-py2.py3-none-any.whl (16 kB)\n",
            "Collecting entrypoints>=0.2.2\n",
            "  Using cached entrypoints-0.4-py3-none-any.whl (5.3 kB)\n",
            "Collecting nbformat>=4.4\n",
            "  Using cached nbformat-5.1.3-py3-none-any.whl (178 kB)\n",
            "Collecting nbclient<0.6.0,>=0.5.0\n",
            "  Using cached nbclient-0.5.9-py3-none-any.whl (69 kB)\n",
            "Collecting nest-asyncio>=1.5\n",
            "  Using cached nest_asyncio-1.5.6-py3-none-any.whl (5.2 kB)\n",
            "Collecting Send2Trash>=1.8.0\n",
            "  Downloading Send2Trash-1.8.0-py3-none-any.whl (18 kB)\n",
            "Collecting pyzmq>=17\n",
            "  Using cached pyzmq-25.0.0-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
            "Collecting argon2-cffi\n",
            "  Downloading argon2_cffi-21.3.0-py3-none-any.whl (14 kB)\n",
            "Collecting prometheus-client\n",
            "  Downloading prometheus_client-0.16.0-py3-none-any.whl (122 kB)\n",
            "     |████████████████████████████████| 122 kB 78.7 MB/s            \n",
            "\u001b[?25hCollecting terminado>=0.8.3\n",
            "  Using cached terminado-0.12.1-py3-none-any.whl (15 kB)\n",
            "Collecting qtpy\n",
            "  Downloading QtPy-2.0.1-py3-none-any.whl (65 kB)\n",
            "     |████████████████████████████████| 65 kB 4.5 MB/s             \n",
            "\u001b[?25hCollecting pickleshare\n",
            "  Using cached pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB)\n",
            "Collecting backcall\n",
            "  Downloading backcall-0.2.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting jedi<=0.17.2,>=0.10\n",
            "  Downloading jedi-0.17.2-py2.py3-none-any.whl (1.4 MB)\n",
            "     |████████████████████████████████| 1.4 MB 63.4 MB/s            \n",
            "\u001b[?25hCollecting pexpect\n",
            "  Using cached pexpect-4.8.0-py2.py3-none-any.whl (59 kB)\n",
            "Collecting async-generator\n",
            "  Using cached async_generator-1.10-py3-none-any.whl (18 kB)\n",
            "Collecting wcwidth\n",
            "  Using cached wcwidth-0.2.6-py2.py3-none-any.whl (29 kB)\n",
            "Collecting ptyprocess\n",
            "  Using cached ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB)\n",
            "Collecting argon2-cffi-bindings\n",
            "  Downloading argon2_cffi_bindings-21.2.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (86 kB)\n",
            "     |████████████████████████████████| 86 kB 6.3 MB/s             \n",
            "\u001b[?25hCollecting webencodings\n",
            "  Using cached webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
            "Collecting zipp>=0.5\n",
            "  Using cached zipp-3.6.0-py3-none-any.whl (5.3 kB)\n",
            "Collecting parso<0.8.0,>=0.7.0\n",
            "  Downloading parso-0.7.1-py2.py3-none-any.whl (109 kB)\n",
            "     |████████████████████████████████| 109 kB 76.6 MB/s            \n",
            "\u001b[?25hBuilding wheels for collected packages: addict, future, mpi4py, posix-ipc, prettytable, pycocotools-fix, pycuda, recordclass, retrying, seaborn, semver, simplejson, tabulate, DLLogger, pytools, fire\n",
            "  Building wheel for addict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for addict: filename=addict-2.1.0-py3-none-any.whl size=2752 sha256=ef0125dd28d737bdc5bd7b53b044df957c3e47a739f0469079a5d48b1238627d\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/98/5a/fdd770697a45732dabdae4e0781dd9bd6c8cd20d552239ea5d\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.17.1-py3-none-any.whl size=488732 sha256=b4da9130712b6ea6c8ae7d5d5226aa78a3423fe9eb10aac40f9e362dfd025c6d\n",
            "  Stored in directory: /root/.cache/pip/wheels/c4/f0/e2/8e4ecc9e1b12a428a7657ba683576d3e79d0a75982f63e8fd2\n",
            "  Building wheel for mpi4py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpi4py: filename=mpi4py-3.0.3-cp36-cp36m-linux_x86_64.whl size=2899552 sha256=3f0472dfb5809dbd884a31f22f55ecd255702b1881a5c1d3d03582ef061c474f\n",
            "  Stored in directory: /root/.cache/pip/wheels/d6/73/83/ad9dd3ebae512829ab3f21657f76403dc4aa6649e1118c9369\n",
            "  Building wheel for posix-ipc (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for posix-ipc: filename=posix_ipc-1.0.4-cp36-cp36m-linux_x86_64.whl size=49137 sha256=bd1b9d3750d06564987f1896f4bfee6738fcccbb98cf0ba83cc187d032bd935f\n",
            "  Stored in directory: /root/.cache/pip/wheels/b3/8a/81/3ec9d2fbdb81eb293433eb68545256d2b0c6f7928d8cc07a8c\n",
            "  Building wheel for prettytable (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for prettytable: filename=prettytable-0.7.2-py3-none-any.whl size=13698 sha256=9b99c58d9bac92b6f42e62dd1c73bd8bff2f7b727af71111eea1177b25bc5d29\n",
            "  Stored in directory: /root/.cache/pip/wheels/11/00/57/e546bd5a658bfe75edfa8584ce66aa7868f742c22cee56ecac\n",
            "  Building wheel for pycocotools-fix (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycocotools-fix: filename=pycocotools_fix-2.0.0.9-cp36-cp36m-linux_x86_64.whl size=361890 sha256=46915a248673ef631c2aee1a578c9481096094bd9dd8349f07340722373c433b\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/c9/3f/3873e31a28358e4931a44b905de397781a6c1c4d9dad4ce9c6\n",
            "  Building wheel for pycuda (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycuda: filename=pycuda-2020.1-cp36-cp36m-linux_x86_64.whl size=639576 sha256=07862f699c8e6ea6050526072837b0ae30bbea3d157b81fe437906e1498bebb1\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/3e/e1/1054e1683e5874983562141f4dd9b5e9f5801b4a8aafab4bef\n",
            "  Building wheel for recordclass (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for recordclass: filename=recordclass-0.13.1-cp36-cp36m-linux_x86_64.whl size=396303 sha256=e8bcef26833a1fc0d26c61327541f744ed4eb9c352929823f69302b887410b4f\n",
            "  Stored in directory: /root/.cache/pip/wheels/52/34/48/9efd640f28b5f334c76a1f7839c997c5d14e62ef8b1db6c35f\n",
            "  Building wheel for retrying (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for retrying: filename=retrying-1.3.3-py3-none-any.whl size=11430 sha256=330e884575d6cea69f16920095e870ea748f20f0c2853ba66e413bf158c41afc\n",
            "  Stored in directory: /root/.cache/pip/wheels/ac/cb/8a/b27bf6323e2f4c462dcbf77d70b7c5e7868a7fbe12871770cf\n",
            "  Building wheel for seaborn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seaborn: filename=seaborn-0.7.1-py3-none-any.whl size=165932 sha256=c9e80401b3756085fcc2a9515c1617fbce7c7a411165f37dcb10af5d6cf38b7c\n",
            "  Stored in directory: /root/.cache/pip/wheels/04/55/ef/83e1521af5f599ab242eaa2195370f1f55a91ae8d1a9d55e13\n",
            "  Building wheel for semver (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for semver: filename=semver-2.7.9-py3-none-any.whl size=5328 sha256=e6d8d9dbd4f203279aa743c1d89a529059309e7cf937fef71b5db3715c523b98\n",
            "  Stored in directory: /root/.cache/pip/wheels/88/6f/02/9e873e44761ca26841913f9f88e2c417676fc857e3a84bdf5d\n",
            "  Building wheel for simplejson (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for simplejson: filename=simplejson-3.13.2-cp36-cp36m-linux_x86_64.whl size=122812 sha256=07a8a0110a9266564ec97c51857f6841e4548e7db734257bda44d94b3b8db4b5\n",
            "  Stored in directory: /root/.cache/pip/wheels/cf/9a/21/686695cc99b86d50397e33d84ae99f61f8e624fcdac9a7ff9e\n",
            "  Building wheel for tabulate (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tabulate: filename=tabulate-0.7.5-py3-none-any.whl size=17958 sha256=f9637403ff810f06c1c356e8f2a6466c74464797b6c4dc843b5b328959cea59f\n",
            "  Stored in directory: /root/.cache/pip/wheels/2e/75/25/b6eca27769907766e4df3c4eaed1afbdad3ce25b14d488a076\n",
            "  Building wheel for DLLogger (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for DLLogger: filename=DLLogger-1.0.0-py3-none-any.whl size=5656 sha256=220c82786ecff7e1175843d3ca03cba4a44a8bd3074e3f79ceec0070b28650fd\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-lzz796ti/wheels/f7/26/a5/529fa71c43e7d45fb465e2548040b16f832a4ab9c54b462bfb\n",
            "  Building wheel for pytools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytools: filename=pytools-2022.1.12-py2.py3-none-any.whl size=65017 sha256=04ecf1ced39fb8679f6eb84c914faba930547b0617119fadda7af7a60d0e34fb\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/b1/56/aa08d4760ce766662705dd7f6fddae17c8d65861d61b2a0f75\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116936 sha256=908d4718b8b9af0243ad1e340d61982f94ef34eca29310d53bbb1c4ca4a21cc0\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/c2/82/39a9e14c600e2c625e0fe6e34deb57f0e2a27a71c1289935ab\n",
            "Successfully built addict future mpi4py posix-ipc prettytable pycocotools-fix pycuda recordclass retrying seaborn semver simplejson tabulate DLLogger pytools fire\n",
            "Installing collected packages: zipp, typing-extensions, six, ipython-genutils, decorator, traitlets, setuptools, pyrsistent, importlib-metadata, attrs, wcwidth, tornado, pyzmq, python-dateutil, pyparsing, pycparser, ptyprocess, parso, nest-asyncio, jupyter-core, jsonschema, entrypoints, webencodings, pygments, prompt-toolkit, pickleshare, pexpect, packaging, nbformat, MarkupSafe, jupyter-client, jedi, cffi, backcall, async-generator, testpath, pandocfilters, nbclient, mistune, jupyterlab-pygments, jinja2, ipython, defusedxml, dataclasses, bleach, argon2-cffi-bindings, terminado, Send2Trash, prometheus-client, numpy, nbconvert, ipykernel, argon2-cffi, urllib3, protobuf, notebook, jmespath, h5py, docutils, widgetsnbextension, termcolor, scipy, qtpy, PyYAML, platformdirs, Pillow, orderedmultidict, onnx, kiwisolver, keras-preprocessing, keras-applications, jupyterlab-widgets, idna, cycler, chardet, certifi, botocore, uritemplate, tifffile, threadpoolctl, s3transfer, requests, qtconsole, PyWavelets, pytz, pytools, pyjwt, psutil, pathlib2, onnxconverter-common, networkx, matplotlib, mako, llvmlite, keras, jupyter-console, joblib, ipywidgets, imageio, future, furl, flatbuffers, fire, cython, appdirs, xmltodict, uplink, uff, tqdm, toposort, tf2onnx, tabulate, simplejson, shapely, semver, seaborn, scikit-learn, scikit-image, retrying, requests-toolbelt, recordclass, pycuda, pycocotools-fix, pyarrow, prettytable, posix-ipc, pandas, opencv-python, onnxruntime, onnx-graphsurgeon, nvidia-ml-py, numba, mpi4py, keras2onnx, keras-metrics, jupyter, grpcio, graphsurgeon, DLLogger, cryptography, clearml, boto3, argparse, argcomplete, addict\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "nvidia-tensorflow 1.15.4+nv20.10 requires numpy<1.19.0,>=1.16.0, but you have numpy 1.19.4 which is incompatible.\n",
            "nvidia-tao 4.0.0 requires idna==2.10, but you have idna 2.7 which is incompatible.\n",
            "nvidia-tao 4.0.0 requires six==1.15.0, but you have six 1.13.0 which is incompatible.\n",
            "nvidia-tao 4.0.0 requires tabulate==0.8.7, but you have tabulate 0.7.5 which is incompatible.\n",
            "nvidia-tao 4.0.0 requires urllib3>=1.26.5, but you have urllib3 1.24.3 which is incompatible.\n",
            "google-colab 1.0.0 requires ipykernel~=4.6.0, but you have ipykernel 5.5.6 which is incompatible.\n",
            "google-colab 1.0.0 requires ipython~=5.5.0, but you have ipython 7.16.3 which is incompatible.\n",
            "google-colab 1.0.0 requires notebook~=5.2.0, but you have notebook 6.4.10 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas~=0.24.0, but you have pandas 0.25.3 which is incompatible.\n",
            "google-colab 1.0.0 requires requests~=2.21.0, but you have requests 2.20.1 which is incompatible.\n",
            "google-colab 1.0.0 requires six~=1.12.0, but you have six 1.13.0 which is incompatible.\n",
            "google-colab 1.0.0 requires tornado~=4.5.0, but you have tornado 6.1 which is incompatible.\u001b[0m\n",
            "Successfully installed DLLogger-1.0.0 MarkupSafe-2.0.1 Pillow-8.4.0 PyWavelets-1.1.1 PyYAML-6.0 Send2Trash-1.8.0 addict-2.1.0 appdirs-1.4.4 argcomplete-1.9.4 argon2-cffi-21.3.0 argon2-cffi-bindings-21.2.0 argparse-1.4.0 async-generator-1.10 attrs-22.2.0 backcall-0.2.0 bleach-4.1.0 boto3-1.9.13 botocore-1.12.253 certifi-2022.12.7 cffi-1.15.1 chardet-3.0.4 clearml-1.7.1rc1 cryptography-39.0.0 cycler-0.11.0 cython-0.29.33 dataclasses-0.8 decorator-5.1.1 defusedxml-0.7.1 docutils-0.15.2 entrypoints-0.4 fire-0.5.0 flatbuffers-1.12 furl-2.1.3 future-0.17.1 graphsurgeon-0.4.5 grpcio-1.48.2 h5py-3.1.0 idna-2.10 imageio-2.15.0 importlib-metadata-4.8.3 ipykernel-5.5.6 ipython-7.16.3 ipython-genutils-0.2.0 ipywidgets-7.7.2 jedi-0.17.2 jinja2-3.0.3 jmespath-0.10.0 joblib-1.1.1 jsonschema-3.2.0 jupyter-1.0.0 jupyter-client-7.1.2 jupyter-console-6.4.3 jupyter-core-4.9.2 jupyterlab-pygments-0.1.2 jupyterlab-widgets-1.1.1 keras-2.2.4 keras-applications-1.0.8 keras-metrics-1.1.0 keras-preprocessing-1.1.2 keras2onnx-1.7.0 kiwisolver-1.3.1 llvmlite-0.32.0 mako-1.1.6 matplotlib-3.3.4 mistune-0.8.4 mpi4py-3.0.3 nbclient-0.5.9 nbconvert-6.0.7 nbformat-5.1.3 nest-asyncio-1.5.6 networkx-2.5.1 notebook-6.4.10 numba-0.47.0 numpy-1.19.4 nvidia-ml-py-11.525.84 onnx-1.8.1 onnx-graphsurgeon-0.3.25 onnxconverter-common-1.13.0 onnxruntime-1.8.0 opencv-python-4.2.0.32 orderedmultidict-1.0.1 packaging-21.3 pandas-0.25.3 pandocfilters-1.5.0 parso-0.7.1 pathlib2-2.3.7.post1 pexpect-4.8.0 pickleshare-0.7.5 platformdirs-2.4.0 posix-ipc-1.0.4 prettytable-0.7.2 prometheus-client-0.16.0 prompt-toolkit-3.0.36 protobuf-3.19.6 psutil-5.9.4 ptyprocess-0.7.0 pyarrow-0.11.1 pycocotools-fix-2.0.0.9 pycparser-2.21 pycuda-2020.1 pygments-2.14.0 pyjwt-2.4.0 pyparsing-3.0.9 pyrsistent-0.18.0 python-dateutil-2.8.2 pytools-2022.1.12 pytz-2022.7.1 pyzmq-25.0.0 qtconsole-5.2.2 qtpy-2.0.1 recordclass-0.13.1 requests-2.27.1 requests-toolbelt-0.8.0 retrying-1.3.3 s3transfer-0.1.13 scikit-image-0.17.2 scikit-learn-0.24.2 scipy-1.5.4 seaborn-0.7.1 semver-2.7.9 setuptools-59.6.0 shapely-1.8.5.post1 simplejson-3.13.2 six-1.15.0 tabulate-0.8.7 termcolor-1.1.0 terminado-0.12.1 testpath-0.6.0 tf2onnx-1.9.2 threadpoolctl-3.1.0 tifffile-2020.9.3 toposort-1.5 tornado-6.1 tqdm-4.19.5 traitlets-4.3.3 typing-extensions-4.1.1 uff-0.6.9 uplink-0.5.5 uritemplate-4.1.1 urllib3-1.26.14 wcwidth-0.2.6 webencodings-0.5.1 widgetsnbextension-3.6.1 xmltodict-0.12.0 zipp-3.6.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/, https://pypi.ngc.nvidia.com\n",
            "Collecting nvidia-tao-tf1==4.0.0.657.dev0\n",
            "  Downloading nvidia_tao_tf1-4.0.0.657.dev0-py3-none-any.whl (14.7 MB)\n",
            "     |████████████████████████████████| 14.7 MB 250 kB/s            \n",
            "\u001b[?25hInstalling collected packages: nvidia-tao-tf1\n",
            "Successfully installed nvidia-tao-tf1-4.0.0.657.dev0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "if os.environ[\"GOOGLE_COLAB\"] == \"1\":\n",
        "    os.environ[\"bash_script\"] = \"setup_env.sh\"\n",
        "else:\n",
        "    os.environ[\"bash_script\"] = \"setup_env_desktop.sh\"\n",
        "\n",
        "!sed -i \"s|PATH_TO_COLAB_NOTEBOOKS|$COLAB_NOTEBOOKS_PATH|g\" $COLAB_NOTEBOOKS_PATH/tensorflow/$bash_script\n",
        "\n",
        "!sh $COLAB_NOTEBOOKS_PATH/tensorflow/$bash_script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Rlf2Tsm3vhgp"
      },
      "outputs": [],
      "source": [
        "if os.environ.get(\"PYTHONPATH\",\"\") == \"\":\n",
        "    os.environ[\"PYTHONPATH\"] = \"\"\n",
        "os.environ[\"PYTHONPATH\"]+=\":/opt/nvidia/\"\n",
        "if os.environ[\"GOOGLE_COLAB\"] == \"1\":\n",
        "    os.environ[\"PYTHONPATH\"]+=\":/usr/local/lib/python3.6/dist-packages/third_party/nvml\"\n",
        "else:\n",
        "    os.environ[\"PYTHONPATH\"]+=\":/home_duplicate/rarunachalam/miniconda3/envs/tf_py_36/lib/python3.6/site-packages/third_party/nvml\" # FIX MINICONDA PATH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fl8fSfXseED3"
      },
      "source": [
        "### 2.4 Reset env variables (Use the same paths which was set in Step 0) <a class=\"anchor\" id=\"head-2-4\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "l_T2vBdzeIcO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bb38812-403f-4b25-94d3-2d600ca8c774"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: TAO_DOCKER_DISABLE=1\n",
            "env: KEY=nvidia_tlt\n",
            "env: NUM_GPUS=1\n",
            "env: GPU_INDEX=0\n",
            "env: COLAB_NOTEBOOKS_PATH=/content/drive/MyDrive/nvidia-tao\n",
            "env: EXPERIMENT_DIR=/content/drive/MyDrive/results/lprnet\n",
            "env: DATA_DIR=/content/drive/MyDrive/lprnet_data/\n",
            "env: SPECS_DIR=/content/drive/MyDrive/nvidia-tao/tensorflow/lprnet/specs\n",
            "total 2\n",
            "-rw------- 1 root root   70 Jan 30 08:19 us_lp_characters.txt\n",
            "-rw------- 1 root root 1137 Jan 30 08:19 tutorial_spec.txt\n"
          ]
        }
      ],
      "source": [
        "# Setting up env variables for cleaner command line commands.\n",
        "import os\n",
        "\n",
        "%env TAO_DOCKER_DISABLE=1\n",
        "\n",
        "%env KEY=nvidia_tlt\n",
        "%env NUM_GPUS=1\n",
        "%env GPU_INDEX=0\n",
        "\n",
        "# Change the paths according to your directory structure, these are just examples\n",
        "%env COLAB_NOTEBOOKS_PATH=/content/drive/MyDrive/nvidia-tao\n",
        "if not os.path.exists(os.environ[\"COLAB_NOTEBOOKS_PATH\"]):\n",
        "    raise Exception(\"Error, enter the path of the colab notebooks repo correctly\")\n",
        "%env EXPERIMENT_DIR=/content/drive/MyDrive/results/lprnet\n",
        "%env DATA_DIR=/content/drive/MyDrive/lprnet_data/\n",
        "\n",
        "SPECS_DIR=f\"{os.environ['COLAB_NOTEBOOKS_PATH']}/tensorflow/lprnet/specs\"\n",
        "%env SPECS_DIR={SPECS_DIR}\n",
        "# Showing list of specification files.\n",
        "!ls -rlt $SPECS_DIR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7N3YWk16Pyoz"
      },
      "source": [
        "## 3. Provide training specification <a class=\"anchor\" id=\"head-3\"></a>\n",
        "\n",
        "* Note the spec $SPEC_DIR/default_sepc.txt is for training on US license plates:\n",
        "    * the max license plate length is 8;\n",
        "        * You can change `max_label_length` in `lpr_config` to satisfy your own dataset.\n",
        "    * the characters of US license plates are: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, A, B, C, D, E, F, G, H, I, J, K, L, M, N, P, Q, R, S, T, U, V, W, X, Y, Z \n",
        "        * You can change `characters_list_file` in `dataset_config` to set your own characters.\n",
        "        * `characters_list_file` should contain all the characters in dataset. And one character takes one line. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "v3fLUuoaPyo0",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e6f4134-d71b-4e6f-a92a-c9eb53066ec4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "random_seed: 42\n",
            "lpr_config {\n",
            "  hidden_units: 512\n",
            "  max_label_length: 8\n",
            "  arch: \"baseline\"\n",
            "  nlayers: 18 #setting nlayers to be 10 to use baseline10 model\n",
            "}\n",
            "training_config {\n",
            "  batch_size_per_gpu: 32\n",
            "  num_epochs: 24\n",
            "  learning_rate {\n",
            "  soft_start_annealing_schedule {\n",
            "    min_learning_rate: 1e-6\n",
            "    max_learning_rate: 1e-5\n",
            "    soft_start: 0.001\n",
            "    annealing: 0.5\n",
            "  }\n",
            "  }\n",
            "  regularizer {\n",
            "    type: L2\n",
            "    weight: 5e-4\n",
            "  }\n",
            "}\n",
            "eval_config {\n",
            "  validation_period_during_training: 5\n",
            "  batch_size: 1\n",
            "}\n",
            "augmentation_config {\n",
            "    output_width: 96\n",
            "    output_height: 48\n",
            "    output_channel: 3\n",
            "    max_rotate_degree: 5\n",
            "    rotate_prob: 0.5\n",
            "    gaussian_kernel_size: 5\n",
            "    gaussian_kernel_size: 7\n",
            "    gaussian_kernel_size: 15\n",
            "    blur_prob: 0.5\n",
            "    reverse_color_prob: 0.5\n",
            "    keep_original_prob: 0.3\n",
            "}\n",
            "dataset_config {\n",
            "  data_sources: {\n",
            "    label_directory_path: \"/content/drive/MyDrive/lprnet_data///train/label\"\n",
            "    image_directory_path: \"/content/drive/MyDrive/lprnet_data///train/image\"\n",
            "  }\n",
            "  characters_list_file: \"/content/drive/MyDrive/nvidia-tao/tensorflow/lprnet/specs//us_lp_characters.txt\"\n",
            "  validation_data_sources: {\n",
            "    label_directory_path: \"/content/drive/MyDrive/lprnet_data///val/label\"\n",
            "    image_directory_path: \"/content/drive/MyDrive/lprnet_data///val/image\"\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "!sed -i \"s|TAO_DATA_PATH|$DATA_DIR/|g\" $SPECS_DIR/tutorial_spec.txt\n",
        "!sed -i \"s|TAO_SPEC_DIR|$SPECS_DIR/|g\" $SPECS_DIR/tutorial_spec.txt\n",
        "!cat $SPECS_DIR/tutorial_spec.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Yb9OAcd7Pyo0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "104c1d62-807c-4eb5-ab92-31df9d85e192"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "A\n",
            "B\n",
            "C\n",
            "D\n",
            "E\n",
            "F\n",
            "G\n",
            "H\n",
            "I\n",
            "J\n",
            "K\n",
            "L\n",
            "M\n",
            "N\n",
            "P\n",
            "Q\n",
            "R\n",
            "S\n",
            "T\n",
            "U\n",
            "V\n",
            "W\n",
            "X\n",
            "Y\n",
            "Z\n"
          ]
        }
      ],
      "source": [
        "!cat $SPECS_DIR/us_lp_characters.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XifdFs5Pyo0"
      },
      "source": [
        "## 4. Run TAO training <a class=\"anchor\" id=\"head-4\"></a>\n",
        "* Provide the sample spec file and the output directory location for models\n",
        "* WARNING: training will take several hours or one day to complete"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "OnhmABuWPyo0"
      },
      "outputs": [],
      "source": [
        "!mkdir -p $EXPERIMENT_DIR/experiment_dir_unpruned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "c8wKuOHNPyo1",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0249316b-a912-4d56-ff4c-e11774bc973d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For multi-GPU, change --gpus based on your machine.\n",
            "Using TensorFlow backend.\n",
            "2023-01-30 08:49:59.750062: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "WARNING:tensorflow:Deprecation warnings have been disabled. Set TF_ENABLE_DEPRECATION_WARNINGS=1 to re-enable them.\n",
            "2023-01-30 08:50:05.584527: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "WARNING:tensorflow:Deprecation warnings have been disabled. Set TF_ENABLE_DEPRECATION_WARNINGS=1 to re-enable them.\n",
            "Using TensorFlow backend.\n",
            "2023-01-30 08:50:08.334592: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2199995000 Hz\n",
            "2023-01-30 08:50:08.334789: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x79101c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2023-01-30 08:50:08.334840: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2023-01-30 08:50:08.336513: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
            "2023-01-30 08:50:08.516095: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1086] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-01-30 08:50:08.516389: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7910540 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2023-01-30 08:50:08.516430: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2023-01-30 08:50:08.516639: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1086] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-01-30 08:50:08.516785: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1665] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2023-01-30 08:50:08.516843: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "2023-01-30 08:50:08.518744: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
            "2023-01-30 08:50:08.519785: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
            "2023-01-30 08:50:08.520185: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
            "2023-01-30 08:50:08.560283: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.11\n",
            "2023-01-30 08:50:08.577498: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
            "2023-01-30 08:50:08.577726: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
            "2023-01-30 08:50:08.577874: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1086] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-01-30 08:50:08.578089: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1086] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-01-30 08:50:08.578232: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1793] Adding visible gpu devices: 0\n",
            "2023-01-30 08:50:08.578276: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "2023-01-30 08:50:08.912790: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1206] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2023-01-30 08:50:08.912853: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1212]      0 \n",
            "2023-01-30 08:50:08.912864: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1225] 0:   N \n",
            "2023-01-30 08:50:08.913145: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1086] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-01-30 08:50:08.913396: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1086] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-01-30 08:50:08.913564: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1351] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13754 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "INFO: Merging specification from /content/drive/MyDrive/nvidia-tao/tensorflow/lprnet/specs/tutorial_spec.txt\n",
            "INFO: Loading pretrained weights. This may take a while...\n",
            "2023-01-30 08:50:16.403256: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 29491200 exceeds 10% of system memory.\n",
            "2023-01-30 08:50:16.436375: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 29491200 exceeds 10% of system memory.\n",
            "2023-01-30 08:50:16.490071: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 29491200 exceeds 10% of system memory.\n",
            "2023-01-30 08:50:16.498576: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 29491200 exceeds 10% of system memory.\n",
            "2023-01-30 08:50:16.510510: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 29491200 exceeds 10% of system memory.\n",
            "Layers that load weights from the pretrained model: ['conv1', 'bn_conv1', 'res2a_branch2a', 'bn2a_branch2a', 'res2a_branch1', 'res2a_branch2b', 'bn2a_branch1', 'bn2a_branch2b', 'res2b_branch2a', 'bn2b_branch2a', 'res2b_branch2b', 'bn2b_branch2b', 'res3a_branch2a', 'bn3a_branch2a', 'res3a_branch1', 'res3a_branch2b', 'bn3a_branch1', 'bn3a_branch2b', 'res3b_branch2a', 'bn3b_branch2a', 'res3b_branch2b', 'bn3b_branch2b', 'res4a_branch2a', 'bn4a_branch2a', 'res4a_branch1', 'res4a_branch2b', 'bn4a_branch1', 'bn4a_branch2b', 'res4b_branch2a', 'bn4b_branch2a', 'res4b_branch2b', 'bn4b_branch2b', 'res5a_branch2a', 'bn5a_branch2a', 'res5a_branch1', 'res5a_branch2b', 'bn5a_branch1', 'bn5a_branch2b', 'res5b_branch2a', 'bn5b_branch2a', 'res5b_branch2b', 'bn5b_branch2b', 'lstm', 'td_dense']\n",
            "Initialize optimizer\n",
            "Model: \"lpnet_baseline_18\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "image_input (InputLayer)        [(None, 3, 48, 96)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Sum (TensorFlowOpLa [(None, 1, 48, 96)]  0           image_input[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1 (Conv2D)                  (None, 64, 48, 96)   640         tf_op_layer_Sum[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "bn_conv1 (BatchNormalization)   (None, 64, 48, 96)   256         conv1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "re_lu (ReLU)                    (None, 64, 48, 96)   0           bn_conv1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D)    (None, 64, 48, 96)   0           re_lu[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "res2a_branch2a (Conv2D)         (None, 64, 48, 96)   36928       max_pooling2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn2a_branch2a (BatchNormalizati (None, 64, 48, 96)   256         res2a_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_1 (ReLU)                  (None, 64, 48, 96)   0           bn2a_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res2a_branch1 (Conv2D)          (None, 64, 48, 96)   4160        max_pooling2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res2a_branch2b (Conv2D)         (None, 64, 48, 96)   36928       re_lu_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bn2a_branch1 (BatchNormalizatio (None, 64, 48, 96)   256         res2a_branch1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn2a_branch2b (BatchNormalizati (None, 64, 48, 96)   256         res2a_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_add (TensorFlowOpLa [(None, 64, 48, 96)] 0           bn2a_branch1[0][0]               \n",
            "                                                                 bn2a_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_2 (ReLU)                  (None, 64, 48, 96)   0           tf_op_layer_add[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "res2b_branch2a (Conv2D)         (None, 64, 48, 96)   36928       re_lu_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bn2b_branch2a (BatchNormalizati (None, 64, 48, 96)   256         res2b_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_3 (ReLU)                  (None, 64, 48, 96)   0           bn2b_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res2b_branch2b (Conv2D)         (None, 64, 48, 96)   36928       re_lu_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bn2b_branch2b (BatchNormalizati (None, 64, 48, 96)   256         res2b_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_add_1 (TensorFlowOp [(None, 64, 48, 96)] 0           re_lu_2[0][0]                    \n",
            "                                                                 bn2b_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_4 (ReLU)                  (None, 64, 48, 96)   0           tf_op_layer_add_1[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "res3a_branch2a (Conv2D)         (None, 128, 24, 48)  73856       re_lu_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bn3a_branch2a (BatchNormalizati (None, 128, 24, 48)  512         res3a_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_5 (ReLU)                  (None, 128, 24, 48)  0           bn3a_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res3a_branch1 (Conv2D)          (None, 128, 24, 48)  8320        re_lu_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "res3a_branch2b (Conv2D)         (None, 128, 24, 48)  147584      re_lu_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bn3a_branch1 (BatchNormalizatio (None, 128, 24, 48)  512         res3a_branch1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn3a_branch2b (BatchNormalizati (None, 128, 24, 48)  512         res3a_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_add_2 (TensorFlowOp [(None, 128, 24, 48) 0           bn3a_branch1[0][0]               \n",
            "                                                                 bn3a_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_6 (ReLU)                  (None, 128, 24, 48)  0           tf_op_layer_add_2[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "res3b_branch2a (Conv2D)         (None, 128, 24, 48)  147584      re_lu_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bn3b_branch2a (BatchNormalizati (None, 128, 24, 48)  512         res3b_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_7 (ReLU)                  (None, 128, 24, 48)  0           bn3b_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res3b_branch2b (Conv2D)         (None, 128, 24, 48)  147584      re_lu_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bn3b_branch2b (BatchNormalizati (None, 128, 24, 48)  512         res3b_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_add_3 (TensorFlowOp [(None, 128, 24, 48) 0           re_lu_6[0][0]                    \n",
            "                                                                 bn3b_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_8 (ReLU)                  (None, 128, 24, 48)  0           tf_op_layer_add_3[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "res4a_branch2a (Conv2D)         (None, 256, 12, 24)  295168      re_lu_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bn4a_branch2a (BatchNormalizati (None, 256, 12, 24)  1024        res4a_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_9 (ReLU)                  (None, 256, 12, 24)  0           bn4a_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4a_branch1 (Conv2D)          (None, 256, 12, 24)  33024       re_lu_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "res4a_branch2b (Conv2D)         (None, 256, 12, 24)  590080      re_lu_9[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bn4a_branch1 (BatchNormalizatio (None, 256, 12, 24)  1024        res4a_branch1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4a_branch2b (BatchNormalizati (None, 256, 12, 24)  1024        res4a_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_add_4 (TensorFlowOp [(None, 256, 12, 24) 0           bn4a_branch1[0][0]               \n",
            "                                                                 bn4a_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_10 (ReLU)                 (None, 256, 12, 24)  0           tf_op_layer_add_4[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "res4b_branch2a (Conv2D)         (None, 256, 12, 24)  590080      re_lu_10[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "bn4b_branch2a (BatchNormalizati (None, 256, 12, 24)  1024        res4b_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_11 (ReLU)                 (None, 256, 12, 24)  0           bn4b_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4b_branch2b (Conv2D)         (None, 256, 12, 24)  590080      re_lu_11[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "bn4b_branch2b (BatchNormalizati (None, 256, 12, 24)  1024        res4b_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_add_5 (TensorFlowOp [(None, 256, 12, 24) 0           re_lu_10[0][0]                   \n",
            "                                                                 bn4b_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_12 (ReLU)                 (None, 256, 12, 24)  0           tf_op_layer_add_5[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "res5a_branch2a (Conv2D)         (None, 300, 12, 24)  691500      re_lu_12[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "bn5a_branch2a (BatchNormalizati (None, 300, 12, 24)  1200        res5a_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_13 (ReLU)                 (None, 300, 12, 24)  0           bn5a_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res5a_branch1 (Conv2D)          (None, 300, 12, 24)  77100       re_lu_12[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "res5a_branch2b (Conv2D)         (None, 300, 12, 24)  810300      re_lu_13[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "bn5a_branch1 (BatchNormalizatio (None, 300, 12, 24)  1200        res5a_branch1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn5a_branch2b (BatchNormalizati (None, 300, 12, 24)  1200        res5a_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_add_6 (TensorFlowOp [(None, 300, 12, 24) 0           bn5a_branch1[0][0]               \n",
            "                                                                 bn5a_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_14 (ReLU)                 (None, 300, 12, 24)  0           tf_op_layer_add_6[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "res5b_branch2a (Conv2D)         (None, 300, 12, 24)  810300      re_lu_14[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "bn5b_branch2a (BatchNormalizati (None, 300, 12, 24)  1200        res5b_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_15 (ReLU)                 (None, 300, 12, 24)  0           bn5b_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res5b_branch2b (Conv2D)         (None, 300, 12, 24)  810300      re_lu_15[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "bn5b_branch2b (BatchNormalizati (None, 300, 12, 24)  1200        res5b_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_add_7 (TensorFlowOp [(None, 300, 12, 24) 0           re_lu_14[0][0]                   \n",
            "                                                                 bn5b_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_16 (ReLU)                 (None, 300, 12, 24)  0           tf_op_layer_add_7[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "permute_feature (Permute)       (None, 24, 12, 300)  0           re_lu_16[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "flatten_feature (Reshape)       (None, 24, 3600)     0           permute_feature[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     (None, 24, 512)      8423424     flatten_feature[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "td_dense (TimeDistributed)      (None, 24, 36)       18468       lstm[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "softmax (Softmax)               (None, 24, 36)       0           td_dense[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 14,432,480\n",
            "Trainable params: 14,424,872\n",
            "Non-trainable params: 7,608\n",
            "__________________________________________________________________________________________________\n",
            "INFO: Number of images in the training dataset:\t   111\n",
            "INFO: Number of images in the validation dataset:\t   110\n",
            "INFO: Log file already exists at /content/drive/MyDrive/results/lprnet/experiment_dir_unpruned/status.json\n",
            "INFO: Starting Training Loop.\n",
            "Epoch 1/24\n",
            "2023-01-30 08:50:36.685017: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
            "2023-01-30 08:50:38.267412: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
            "1/4 [======>.......................] - ETA: 50s - loss: 1.0337WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (1.072417). Check your callbacks.\n",
            "WARNING: Method (on_train_batch_end) is slow compared to the batch update (1.072417). Check your callbacks.\n",
            "3/4 [=====================>........] - ETA: 5s - loss: 0.7975 INFO: Training loop in progress\n",
            "4/4 [==============================] - 20s 5s/step - loss: 0.7179\n",
            "Epoch 2/24\n",
            "3/4 [=====================>........] - ETA: 0s - loss: 0.8549INFO: Training loop in progress\n",
            "4/4 [==============================] - 1s 317ms/step - loss: 0.8717\n",
            "Epoch 3/24\n",
            "3/4 [=====================>........] - ETA: 0s - loss: 0.6054INFO: Training loop in progress\n",
            "4/4 [==============================] - 1s 186ms/step - loss: 0.5439\n",
            "Epoch 4/24\n",
            "3/4 [=====================>........] - ETA: 0s - loss: 0.8812INFO: Training loop in progress\n",
            "4/4 [==============================] - 1s 212ms/step - loss: 0.8555\n",
            "Epoch 5/24\n",
            "3/4 [=====================>........] - ETA: 0s - loss: 0.3072\n",
            "Epoch 00005: saving model to /content/drive/MyDrive/results/lprnet/experiment_dir_unpruned/weights/lprnet_epoch-05.tlt\n",
            "\n",
            "\n",
            "*******************************************\n",
            "Accuracy: 93 / 110  0.8454545454545455\n",
            "*******************************************\n",
            "\n",
            "\n",
            "INFO: Evaluation metrics generated.\n",
            "INFO: Training loop in progress\n",
            "4/4 [==============================] - 15s 4s/step - loss: 0.4012\n",
            "Epoch 6/24\n",
            "3/4 [=====================>........] - ETA: 0s - loss: 0.3414INFO: Training loop in progress\n",
            "4/4 [==============================] - 1s 194ms/step - loss: 0.4917\n",
            "Epoch 7/24\n",
            "3/4 [=====================>........] - ETA: 0s - loss: 0.4048INFO: Training loop in progress\n",
            "4/4 [==============================] - 1s 216ms/step - loss: 0.3609\n",
            "Epoch 8/24\n",
            "3/4 [=====================>........] - ETA: 0s - loss: 0.7095INFO: Training loop in progress\n",
            "4/4 [==============================] - 1s 219ms/step - loss: 0.5736\n",
            "Epoch 9/24\n",
            "3/4 [=====================>........] - ETA: 0s - loss: 0.5923INFO: Training loop in progress\n",
            "4/4 [==============================] - 1s 218ms/step - loss: 0.4747\n",
            "Epoch 10/24\n",
            "3/4 [=====================>........] - ETA: 0s - loss: 0.3280\n",
            "Epoch 00010: saving model to /content/drive/MyDrive/results/lprnet/experiment_dir_unpruned/weights/lprnet_epoch-10.tlt\n",
            "\n",
            "\n",
            "*******************************************\n",
            "Accuracy: 97 / 110  0.8818181818181818\n",
            "*******************************************\n",
            "\n",
            "\n",
            "INFO: Evaluation metrics generated.\n",
            "INFO: Training loop in progress\n",
            "4/4 [==============================] - 6s 1s/step - loss: 0.3489\n",
            "Epoch 11/24\n",
            "3/4 [=====================>........] - ETA: 0s - loss: 0.2353INFO: Training loop in progress\n",
            "4/4 [==============================] - 1s 182ms/step - loss: 0.2516\n",
            "Epoch 12/24\n",
            "3/4 [=====================>........] - ETA: 0s - loss: 0.2661INFO: Training loop in progress\n",
            "4/4 [==============================] - 1s 215ms/step - loss: 0.2766\n",
            "Epoch 13/24\n",
            "3/4 [=====================>........] - ETA: 0s - loss: 0.4717INFO: Training loop in progress\n",
            "4/4 [==============================] - 1s 224ms/step - loss: 0.3865\n",
            "Epoch 14/24\n",
            "3/4 [=====================>........] - ETA: 0s - loss: 0.2503INFO: Training loop in progress\n",
            "4/4 [==============================] - 1s 222ms/step - loss: 0.3041\n",
            "Epoch 15/24\n",
            "3/4 [=====================>........] - ETA: 0s - loss: 0.2498\n",
            "Epoch 00015: saving model to /content/drive/MyDrive/results/lprnet/experiment_dir_unpruned/weights/lprnet_epoch-15.tlt\n",
            "\n",
            "\n",
            "*******************************************\n",
            "Accuracy: 96 / 110  0.8727272727272727\n",
            "*******************************************\n",
            "\n",
            "\n",
            "INFO: Evaluation metrics generated.\n",
            "INFO: Training loop in progress\n",
            "4/4 [==============================] - 6s 2s/step - loss: 0.3379\n",
            "Epoch 16/24\n",
            "3/4 [=====================>........] - ETA: 0s - loss: 0.2043INFO: Training loop in progress\n",
            "4/4 [==============================] - 1s 187ms/step - loss: 0.2310\n",
            "Epoch 17/24\n",
            "3/4 [=====================>........] - ETA: 0s - loss: 0.2680INFO: Training loop in progress\n",
            "4/4 [==============================] - 1s 214ms/step - loss: 0.2454\n",
            "Epoch 18/24\n",
            "3/4 [=====================>........] - ETA: 0s - loss: 0.2925INFO: Training loop in progress\n",
            "4/4 [==============================] - 1s 213ms/step - loss: 0.2691\n",
            "Epoch 19/24\n",
            "3/4 [=====================>........] - ETA: 0s - loss: 0.2174INFO: Training loop in progress\n",
            "4/4 [==============================] - 1s 231ms/step - loss: 0.2090\n",
            "Epoch 20/24\n",
            "3/4 [=====================>........] - ETA: 0s - loss: 0.2118\n",
            "Epoch 00020: saving model to /content/drive/MyDrive/results/lprnet/experiment_dir_unpruned/weights/lprnet_epoch-20.tlt\n",
            "\n",
            "\n",
            "*******************************************\n",
            "Accuracy: 96 / 110  0.8727272727272727\n",
            "*******************************************\n",
            "\n",
            "\n",
            "INFO: Evaluation metrics generated.\n",
            "INFO: Training loop in progress\n",
            "4/4 [==============================] - 6s 2s/step - loss: 0.2105\n",
            "Epoch 21/24\n",
            "3/4 [=====================>........] - ETA: 0s - loss: 0.2117INFO: Training loop in progress\n",
            "4/4 [==============================] - 1s 183ms/step - loss: 0.2231\n",
            "Epoch 22/24\n",
            "3/4 [=====================>........] - ETA: 0s - loss: 0.1930INFO: Training loop in progress\n",
            "4/4 [==============================] - 1s 218ms/step - loss: 0.1981\n",
            "Epoch 23/24\n",
            "3/4 [=====================>........] - ETA: 0s - loss: 0.2659INFO: Training loop in progress\n",
            "4/4 [==============================] - 1s 232ms/step - loss: 0.2511\n",
            "Epoch 24/24\n",
            "3/4 [=====================>........] - ETA: 0s - loss: 0.2267\n",
            "Epoch 00024: saving model to /content/drive/MyDrive/results/lprnet/experiment_dir_unpruned/weights/lprnet_epoch-24.tlt\n",
            "INFO: Training loop in progress\n",
            "4/4 [==============================] - 2s 617ms/step - loss: 0.2251\n",
            "\n",
            "\n",
            "*******************************************\n",
            "Accuracy: 95 / 110  0.8636363636363636\n",
            "*******************************************\n",
            "\n",
            "\n",
            "INFO: Evaluation metrics generated.\n",
            "INFO: Training loop complete.\n",
            "INFO: Training finished successfully.\n",
            "Telemetry data couldn't be sent, but the command ran successfully.\n",
            "[WARNING]: <urlopen error [Errno -2] Name or service not known>\n",
            "Execution status: PASS\n"
          ]
        }
      ],
      "source": [
        "print(\"For multi-GPU, change --gpus based on your machine.\")\n",
        "!tao lprnet train --gpus=1 --gpu_index=$GPU_INDEX \\\n",
        "                  -e $SPECS_DIR/tutorial_spec.txt \\\n",
        "                  -r $EXPERIMENT_DIR/experiment_dir_unpruned \\\n",
        "                  -k $KEY \\\n",
        "                  -m $EXPERIMENT_DIR/pretrained_lprnet_baseline18/lprnet_vtrainable_v1.0/us_lprnet_baseline18_trainable.tlt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "R7I87ZIdPyo1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1591292-a181-48f2-e8ed-cc8c67382956"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To resume training from a checkpoint, set the -m option to be the .tlt you want to resume from and --initial_epochs to be the epoch index of the resumed checkpoint\n"
          ]
        }
      ],
      "source": [
        "print(\"To resume training from a checkpoint, set the -m option to be the .tlt you want to resume from and --initial_epochs to be the epoch index of the resumed checkpoint\")\n",
        "# !tao lprnet train --gpu_index=$GPU_INDEX \\\n",
        "#                   -e $SPECS_DIR/tutorial_spec.txt \\\n",
        "#                   -r $EXPERIMENT_DIR/experiment_dir_unpruned \\\n",
        "#                   -k $KEY \\\n",
        "#                   -m $EXPERIMENT_DIR/experiment_dir_unpruned/weights/lprnet_epoch-01.tlt\n",
        "#                   --initial_epoch 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "WDkWKQQkPyo1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "260f4e28-ad37-4496-ca7e-d281d0c3b892"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model for each epoch:\n",
            "---------------------\n",
            "total 553M\n",
            "-rw------- 1 root root 111M Jan 30 08:51 lprnet_epoch-05.tlt\n",
            "-rw------- 1 root root 111M Jan 30 08:51 lprnet_epoch-10.tlt\n",
            "-rw------- 1 root root 111M Jan 30 08:51 lprnet_epoch-15.tlt\n",
            "-rw------- 1 root root 111M Jan 30 08:51 lprnet_epoch-20.tlt\n",
            "-rw------- 1 root root 111M Jan 30 08:51 lprnet_epoch-24.tlt\n"
          ]
        }
      ],
      "source": [
        "print('Model for each epoch:')\n",
        "print('---------------------')\n",
        "!ls -ltrh $EXPERIMENT_DIR/experiment_dir_unpruned/weights/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCYg-llnPyo1"
      },
      "source": [
        "## 5. Evaluate trained models <a class=\"anchor\" id=\"head-5\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ELV8uJr-Pyo2",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea2d2e5a-38f2-4af9-9f92-766e495a33f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using TensorFlow backend.\n",
            "2023-01-30 09:00:24.838174: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "WARNING:tensorflow:Deprecation warnings have been disabled. Set TF_ENABLE_DEPRECATION_WARNINGS=1 to re-enable them.\n",
            "2023-01-30 09:00:28.686788: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "WARNING:tensorflow:Deprecation warnings have been disabled. Set TF_ENABLE_DEPRECATION_WARNINGS=1 to re-enable them.\n",
            "Using TensorFlow backend.\n",
            "2023-01-30 09:00:31,170 [INFO] root: Starting LPRNet evaluation.\n",
            "2023-01-30 09:00:31,171 [INFO] iva.lprnet.utils.spec_loader: Merging specification from /content/drive/MyDrive/nvidia-tao/tensorflow/lprnet/specs/tutorial_spec.txt\n",
            "2023-01-30 09:00:33.080690: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
            "2023-01-30 09:00:33.085854: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1086] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-01-30 09:00:33.086047: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1665] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2023-01-30 09:00:33.086078: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "2023-01-30 09:00:33.093669: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
            "2023-01-30 09:00:33.097498: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
            "2023-01-30 09:00:33.104040: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
            "2023-01-30 09:00:33.135004: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.11\n",
            "2023-01-30 09:00:33.135690: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
            "2023-01-30 09:00:33.135901: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
            "2023-01-30 09:00:33.136036: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1086] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-01-30 09:00:33.136273: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1086] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-01-30 09:00:33.136403: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1793] Adding visible gpu devices: 0\n",
            "2023-01-30 09:00:33.142069: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2199995000 Hz\n",
            "2023-01-30 09:00:33.142258: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0xb2c6380 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2023-01-30 09:00:33.142284: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2023-01-30 09:00:33.320368: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1086] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-01-30 09:00:33.320634: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0xb2c6540 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2023-01-30 09:00:33.320666: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2023-01-30 09:00:33.320883: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1086] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-01-30 09:00:33.321042: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1665] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2023-01-30 09:00:33.321080: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "2023-01-30 09:00:33.321151: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
            "2023-01-30 09:00:33.321178: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
            "2023-01-30 09:00:33.321201: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
            "2023-01-30 09:00:33.321222: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.11\n",
            "2023-01-30 09:00:33.321243: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
            "2023-01-30 09:00:33.321264: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
            "2023-01-30 09:00:33.321345: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1086] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-01-30 09:00:33.321527: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1086] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-01-30 09:00:33.321666: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1793] Adding visible gpu devices: 0\n",
            "2023-01-30 09:00:33.321727: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "2023-01-30 09:00:33.654506: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1206] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2023-01-30 09:00:33.654567: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1212]      0 \n",
            "2023-01-30 09:00:33.654579: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1225] 0:   N \n",
            "2023-01-30 09:00:33.654827: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1086] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-01-30 09:00:33.655097: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1086] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-01-30 09:00:33.655244: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2023-01-30 09:00:33.655283: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1351] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13754 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "Using TLT model for inference, setting batch size to the one in eval_config: 1\n",
            "Producing predictions:   0% 0/110 [00:00<?, ?it/s]2023-01-30 09:00:37.490837: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
            "2023-01-30 09:00:37.934292: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
            "Producing predictions: 100% 110/110 [00:05<00:00, 20.55it/s]\n",
            "Accuracy: 95 / 110  0.8636363636363636\n",
            "2023-01-30 09:00:42,593 [INFO] root: Evaluation finished successfully.\n",
            "Telemetry data couldn't be sent, but the command ran successfully.\n",
            "[WARNING]: <urlopen error [Errno -2] Name or service not known>\n",
            "Execution status: PASS\n"
          ]
        }
      ],
      "source": [
        "!tao lprnet evaluate --gpu_index=$GPU_INDEX -e $SPECS_DIR/tutorial_spec.txt \\\n",
        "                     -m $EXPERIMENT_DIR/experiment_dir_unpruned/weights/lprnet_epoch-24.tlt \\\n",
        "                     -k $KEY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvZE4H8KPyo2"
      },
      "source": [
        "## 6. Inferences <a class=\"anchor\" id=\"head-6\"></a>\n",
        "In this section, we run the lprnet inference tool to generate inferences on the trained models and print the results. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "YlqyTE7MPyo3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04d9ab32-2c96-4828-80c1-28dd4e9f6194"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using TensorFlow backend.\n",
            "2023-01-30 09:00:54.938683: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "WARNING:tensorflow:Deprecation warnings have been disabled. Set TF_ENABLE_DEPRECATION_WARNINGS=1 to re-enable them.\n",
            "2023-01-30 09:00:59.047166: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "WARNING:tensorflow:Deprecation warnings have been disabled. Set TF_ENABLE_DEPRECATION_WARNINGS=1 to re-enable them.\n",
            "Using TensorFlow backend.\n",
            "INFO: Log file already exists at /content/drive/MyDrive/results/lprnet/experiment_dir_unpruned/weights/status.json\n",
            "INFO: Starting LPRNet Inference.\n",
            "INFO: Merging specification from /content/drive/MyDrive/nvidia-tao/tensorflow/lprnet/specs/tutorial_spec.txt\n",
            "2023-01-30 09:01:03.351731: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
            "2023-01-30 09:01:03.356831: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1086] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-01-30 09:01:03.357010: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1665] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2023-01-30 09:01:03.357046: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "2023-01-30 09:01:03.359641: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
            "2023-01-30 09:01:03.360578: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
            "2023-01-30 09:01:03.360909: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
            "2023-01-30 09:01:03.367356: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.11\n",
            "2023-01-30 09:01:03.367975: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
            "2023-01-30 09:01:03.368159: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
            "2023-01-30 09:01:03.368269: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1086] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-01-30 09:01:03.368479: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1086] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-01-30 09:01:03.368616: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1793] Adding visible gpu devices: 0\n",
            "2023-01-30 09:01:03.374076: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2199995000 Hz\n",
            "2023-01-30 09:01:03.374253: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0xc494380 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2023-01-30 09:01:03.374279: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2023-01-30 09:01:03.545637: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1086] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-01-30 09:01:03.545937: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0xc494540 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2023-01-30 09:01:03.545969: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2023-01-30 09:01:03.546165: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1086] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-01-30 09:01:03.546316: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1665] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2023-01-30 09:01:03.546370: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "2023-01-30 09:01:03.546432: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
            "2023-01-30 09:01:03.546463: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
            "2023-01-30 09:01:03.546489: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
            "2023-01-30 09:01:03.546511: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.11\n",
            "2023-01-30 09:01:03.546536: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
            "2023-01-30 09:01:03.546561: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
            "2023-01-30 09:01:03.546642: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1086] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-01-30 09:01:03.546861: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1086] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-01-30 09:01:03.546997: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1793] Adding visible gpu devices: 0\n",
            "2023-01-30 09:01:03.547054: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "2023-01-30 09:01:03.862102: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1206] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2023-01-30 09:01:03.862163: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1212]      0 \n",
            "2023-01-30 09:01:03.862176: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1225] 0:   N \n",
            "2023-01-30 09:01:03.862419: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1086] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-01-30 09:01:03.862662: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1086] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-01-30 09:01:03.862819: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2023-01-30 09:01:03.862859: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1351] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13754 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "Using TLT model for inference, setting batch size to the one in eval_config: 1\n",
            "2023-01-30 09:01:07.832610: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
            "2023-01-30 09:01:08.265460: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
            "/content/drive/MyDrive/lprnet_data//val/image/wts-lg-000148.jpg:6TIX874 \n",
            "/content/drive/MyDrive/lprnet_data//val/image/wts-lg-000157.jpg:7EAV033 \n",
            "/content/drive/MyDrive/lprnet_data//val/image/0b86cecf-67d1-4fc0-87c9-b36b0ee228bb.jpg:YG9X2G \n",
            "/content/drive/MyDrive/lprnet_data//val/image/e73fd200-7ba4-4725-9d1d-2ec710864df6.jpg:8FV480 \n",
            "/content/drive/MyDrive/lprnet_data//val/image/wts-lg-000126.jpg:6NZT067 \n",
            "/content/drive/MyDrive/lprnet_data//val/image/wts-lg-000076.jpg:4GLS168 \n",
            "/content/drive/MyDrive/lprnet_data//val/image/us6.jpg:HL0KTY1 \n",
            "/content/drive/MyDrive/lprnet_data//val/image/wts-lg-000134.jpg:6WAW786 \n",
            "/content/drive/MyDrive/lprnet_data//val/image/wts-lg-000040.jpg:WD2R9B \n",
            "/content/drive/MyDrive/lprnet_data//val/image/wts-lg-000011.jpg:MF2Y8X \n",
            "/content/drive/MyDrive/lprnet_data//val/image/3850ba91-3c64-4c64-acba-0c46b61ec0da.jpg:HK0E7C \n",
            "/content/drive/MyDrive/lprnet_data//val/image/wts-lg-000068.jpg:N359973 \n",
            "/content/drive/MyDrive/lprnet_data//val/image/wts-lg-000156.jpg:5TZA419 \n",
            "/content/drive/MyDrive/lprnet_data//val/image/wts-lg-000047.jpg:CLT0108 \n",
            "/content/drive/MyDrive/lprnet_data//val/image/car12.jpg:5UVR090 \n",
            "/content/drive/MyDrive/lprnet_data//val/image/a03ced3f-5a97-4e75-8106-fabfd2b8b76e.jpg:08E6CK \n",
            "/content/drive/MyDrive/lprnet_data//val/image/wts-lg-000136.jpg:CYZ5139 \n",
            "/content/drive/MyDrive/lprnet_data//val/image/wts-lg-000010.jpg:HH9G5W \n",
            "/content/drive/MyDrive/lprnet_data//val/image/us8.jpg:EAZ6913 \n",
            "/content/drive/MyDrive/lprnet_data//val/image/wts-lg-000071.jpg:WSG979 \n",
            "/content/drive/MyDrive/lprnet_data//val/image/wts-lg-000102.jpg:DL9E1U \n",
            "/content/drive/MyDrive/lprnet_data//val/image/wts-lg-000013.jpg:D45X9P \n",
            "/content/drive/MyDrive/lprnet_data//val/image/wts-lg-000101.jpg:SH0LRZX \n",
            "/content/drive/MyDrive/lprnet_data//val/image/wts-lg-000117.jpg:D315771 \n",
            "/content/drive/MyDrive/lprnet_data//val/image/wts-lg-000084.jpg:5AEA911 \n",
            "/content/drive/MyDrive/lprnet_data//val/image/wts-lg-000020.jpg:R107555 \n",
            "/content/drive/MyDrive/lprnet_data//val/image/wts-lg-000043.jpg:FB8R4T \n",
            "/content/drive/MyDrive/lprnet_data//val/image/21d8c31d-3deb-494b-9c63-c0223306fd82.jpg:2DA044 \n",
            "/content/drive/MyDrive/lprnet_data//val/image/wts-lg-000048.jpg:FE5R2D \n",
            "/content/drive/MyDrive/lprnet_data//val/image/wts-lg-000032.jpg:21A1BX \n",
            "/content/drive/MyDrive/lprnet_data//val/image/wts-lg-000070.jpg:WXY6184 \n",
            "/content/drive/MyDrive/lprnet_data//val/image/wts-lg-000097.jpg:R589789 \n",
            "/content/drive/MyDrive/lprnet_data//val/image/f0a3b8c0-198a-471b-9ca9-345c3dd28073.jpg:HB8X9J \n",
            "/content/drive/MyDrive/lprnet_data//val/image/wts-lg-000174.jpg:5BDE732 \n",
            "/content/drive/MyDrive/lprnet_data//val/image/wts-lg-000052.jpg:DK7J8G \n",
            "/content/drive/MyDrive/lprnet_data//val/image/wts-lg-000168.jpg:RZN384 \n",
            "/content/drive/MyDrive/lprnet_data//val/image/wts-lg-000031.jpg:FE9N6K \n",
            "/content/drive/MyDrive/lprnet_data//val/image/wts-lg-000145.jpg:4KBZ265 \n",
            "/content/drive/MyDrive/lprnet_data//val/image/wts-lg-000044.jpg:AJK7551 \n",
            "/content/drive/MyDrive/lprnet_data//val/image/wts-lg-000166.jpg:4NZZ935 \n",
            "/content/drive/MyDrive/lprnet_data//val/image/wts-lg-000083.jpg:N2427E \n",
            "/content/drive/MyDrive/lprnet_data//val/image/wts-lg-000029.jpg:4YE132 \n",
            "/content/drive/MyDrive/lprnet_data//val/image/wts-lg-000017.jpg:7FR971 \n",
            "/content/drive/MyDrive/lprnet_data//val/image/car21.jpg:YF2L6G \n",
            "/content/drive/MyDrive/lprnet_data//val/image/wts-lg-000077.jpg:5ZUC921 \n",
            "/content/drive/MyDrive/lprnet_data//val/image/wts-lg-000151.jpg:5RIN962 \n",
            "/content/drive/MyDrive/lprnet_data//val/image/wts-lg-000058.jpg:CEMRNT \n",
            "/content/drive/MyDrive/lprnet_data//val/image/wts-lg-000016.jpg:DL3C2Z \n",
            "/content/drive/MyDrive/lprnet_data//val/image/wts-lg-000082.jpg:CJUJ79 \n",
            "/content/drive/MyDrive/lprnet_data//val/image/wts-lg-000165.jpg:CGT2069 \n",
            "/content/drive/MyDrive/lprnet_data//val/image/wts-lg-000159.jpg:59321P1 \n",
            "/content/drive/MyDrive/lprnet_data//val/image/wts-lg-000078.jpg:5EZP631 \n",
            "/content/drive/MyDrive/lprnet_data//val/image/wts-lg-000187.jpg:UJ5E1A \n",
            "/content/drive/MyDrive/lprnet_data//val/image/wts-lg-000094.jpg:J8P4V \n",
            "/content/drive/MyDrive/lprnet_data//val/image/wts-lg-000067.jpg:8951283 \n",
            "/content/drive/MyDrive/lprnet_data//val/image/wts-lg-000085.jpg:BDJ0529 \n",
            "/content/drive/MyDrive/lprnet_data//val/image/wts-lg-000065.jpg:AC0M7V \n",
            "/content/drive/MyDrive/lprnet_data//val/image/car11.jpg:DG3C8P \n",
            "/content/drive/MyDrive/lprnet_data//val/image/wts-lg-000056.jpg:SZA679 \n",
            "/content/drive/MyDrive/lprnet_data//val/image/wts-lg-000160.jpg:8S16841 \n",
            "/content/drive/MyDrive/lprnet_data//val/image/wts-lg-000045.jpg:HF4Y3B \n",
            "/content/drive/MyDrive/lprnet_data//val/image/car18.jpg:SB3X6N \n",
            "/content/drive/MyDrive/lprnet_data//val/image/wts-lg-000095.jpg:4FCW533 \n",
            "/content/drive/MyDrive/lprnet_data//val/image/wts-lg-000142.jpg:5TGN556 \n",
            "/content/drive/MyDrive/lprnet_data//val/image/wts-lg-000181.jpg:PH5M9Y \n",
            "/content/drive/MyDrive/lprnet_data//val/image/wts-lg-000118.jpg:853FL \n",
            "/content/drive/MyDrive/lprnet_data//val/image/wts-lg-000069.jpg:TWF220 \n",
            "/content/drive/MyDrive/lprnet_data//val/image/wts-lg-000057.jpg:YBW233 \n",
            "/content/drive/MyDrive/lprnet_data//val/image/wts-lg-000188.jpg:UJ3V0J \n",
            "/content/drive/MyDrive/lprnet_data//val/image/wts-lg-000028.jpg:SK2W6K \n",
            "/content/drive/MyDrive/lprnet_data//val/image/33fa5185-0286-4e8f-b775-46162eba39d4.jpg:R82D503 \n",
            "/content/drive/MyDrive/lprnet_data//val/image/wts-lg-000195.jpg:8D79882 \n",
            "/content/drive/MyDrive/lprnet_data//val/image/wts-lg-000143.jpg:3127K \n",
            "/content/drive/MyDrive/lprnet_data//val/image/wts-lg-000173.jpg:5FEE732 \n",
            "/content/drive/MyDrive/lprnet_data//val/image/wts-lg-000030.jpg:FK2X2Z \n",
            "/content/drive/MyDrive/lprnet_data//val/image/wts-lg-000034.jpg:HK5R9C \n",
            "/content/drive/MyDrive/lprnet_data//val/image/wts-lg-000039.jpg:WLX039 \n",
            "/content/drive/MyDrive/lprnet_data//val/image/wts-lg-000046.jpg:BFN8484 \n",
            "/content/drive/MyDrive/lprnet_data//val/image/wts-lg-000053.jpg:TNP222 \n",
            "/content/drive/MyDrive/lprnet_data//val/image/wts-lg-000066.jpg:SGTE5 \n",
            "/content/drive/MyDrive/lprnet_data//val/image/wts-lg-000026.jpg:ML0C2S \n",
            "/content/drive/MyDrive/lprnet_data//val/image/wts-lg-000199.jpg:J868321 \n",
            "/content/drive/MyDrive/lprnet_data//val/image/d4f79480-366a-40b6-ab2c-328bcba705b2.jpg:FB5X8T \n",
            "/content/drive/MyDrive/lprnet_data//val/image/wts-lg-000190.jpg:6RYJ486 \n",
            "/content/drive/MyDrive/lprnet_data//val/image/wts-lg-000012.jpg:CK0A1E \n",
            "/content/drive/MyDrive/lprnet_data//val/image/wts-lg-000024.jpg:P602849 \n",
            "/content/drive/MyDrive/lprnet_data//val/image/wts-lg-000106.jpg:3MVV252 \n",
            "/content/drive/MyDrive/lprnet_data//val/image/wts-lg-000049.jpg:WG4L2M \n",
            "/content/drive/MyDrive/lprnet_data//val/image/car2.jpg:1049338 \n",
            "/content/drive/MyDrive/lprnet_data//val/image/wts-lg-000129.jpg:CRK4732 \n",
            "/content/drive/MyDrive/lprnet_data//val/image/wts-lg-000059.jpg:WXW835 \n",
            "/content/drive/MyDrive/lprnet_data//val/image/wts-lg-000194.jpg:7CID930 \n",
            "/content/drive/MyDrive/lprnet_data//val/image/wts-lg-000162.jpg:CN2X798 \n",
            "/content/drive/MyDrive/lprnet_data//val/image/wts-lg-000089.jpg:YF2U68 \n",
            "/content/drive/MyDrive/lprnet_data//val/image/wts-lg-000161.jpg:75061H1 \n",
            "/content/drive/MyDrive/lprnet_data//val/image/wts-lg-000193.jpg:RZN384 \n",
            "/content/drive/MyDrive/lprnet_data//val/image/wts-lg-000150.jpg:6WHS906 \n",
            "/content/drive/MyDrive/lprnet_data//val/image/7fbfbe28-aecb-45be-bd05-7cf26acb3c5c.jpg:KC4Z3X \n",
            "/content/drive/MyDrive/lprnet_data//val/image/wts-lg-000037.jpg:RDN464 \n",
            "/content/drive/MyDrive/lprnet_data//val/image/wts-lg-000139.jpg:1268205 \n",
            "/content/drive/MyDrive/lprnet_data//val/image/wts-lg-000018.jpg:BG2H2U \n",
            "/content/drive/MyDrive/lprnet_data//val/image/wts-lg-000170.jpg:5FLR236 \n",
            "/content/drive/MyDrive/lprnet_data//val/image/wts-lg-000033.jpg:PK2V6M \n",
            "/content/drive/MyDrive/lprnet_data//val/image/wts-lg-000100.jpg:SH0RZX \n",
            "/content/drive/MyDrive/lprnet_data//val/image/wts-lg-000074.jpg:AG6V1W \n",
            "/content/drive/MyDrive/lprnet_data//val/image/wts-lg-000123.jpg:6XIU641 \n",
            "/content/drive/MyDrive/lprnet_data//val/image/wts-lg-000177.jpg:6ZWF846 \n",
            "/content/drive/MyDrive/lprnet_data//val/image/wts-lg-000171.jpg:DCK6344 \n",
            "/content/drive/MyDrive/lprnet_data//val/image/wts-lg-000132.jpg:7AHN739 \n",
            "/content/drive/MyDrive/lprnet_data//val/image/wts-lg-000014.jpg:DG276U \n",
            "INFO: Inference finished successfully.\n",
            "Telemetry data couldn't be sent, but the command ran successfully.\n",
            "[WARNING]: <urlopen error [Errno -2] Name or service not known>\n",
            "Execution status: PASS\n"
          ]
        }
      ],
      "source": [
        "# Running inference for detection on n images\n",
        "!tao lprnet inference --gpu_index=$GPU_INDEX -i $DATA_DIR/val/image \\\n",
        "                      -e $SPECS_DIR/tutorial_spec.txt \\\n",
        "                      -m $EXPERIMENT_DIR/experiment_dir_unpruned/weights/lprnet_epoch-24.tlt \\\n",
        "                      -k $KEY "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "lprnet.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.2 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}